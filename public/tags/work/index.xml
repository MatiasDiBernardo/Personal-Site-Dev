<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Work on Matias Di Bernardo</title>
        <link>http://localhost:1313/tags/work/</link>
        <description>Recent content in Work on Matias Di Bernardo</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Matías Di Bernardo</copyright>
        <lastBuildDate>Mon, 24 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/work/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Generative AI, Flow Matching and TTS</title>
        <link>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</link>
        <pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</guid>
        <description>&lt;img src="http://localhost:1313/p/generative-ai-flow-matching-and-tts/front2.PNG" alt="Featured image of post Generative AI, Flow Matching and TTS" /&gt;&lt;p&gt;At &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we continuously work with state-of-the-art &lt;em&gt;Deep Learning&lt;/em&gt; models. To achieve this, we stay up to date with the latest developments in the field. In this presentation, the objective was to explain the fundamentals of the &lt;em&gt;F5-TTS&lt;/em&gt; model, a cutting-edge speech synthesis system. During the session, the concept of generative artificial intelligence and flow-based models was introduced, as &lt;em&gt;F5-TTS&lt;/em&gt; is based on this technique. The purpose is to share this knowledge with the team and with anyone interested in getting started with &lt;em&gt;Text-to-Speech&lt;/em&gt; (TTS) models.&lt;/p&gt;
&lt;h2 id=&#34;introduction---generative-ai-and-tts&#34;&gt;Introduction - Generative AI and TTS
&lt;/h2&gt;&lt;p&gt;The class was structured as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to TTS models&lt;/strong&gt;: Description of the different models used, their main characteristics, and the reasons behind changing approaches.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evolution of TTS models&lt;/strong&gt;: A graph-based survey illustrating the progression of various TTS technologies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paradigm shift in language models&lt;/strong&gt;: Explanation of how using large datasets has enabled better generalization in language models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example of generalization&lt;/strong&gt;: Illustrative cases, such as generating an image of an astronaut riding a horse, audio examples, and synthesis of specific accents (e.g., an angry Cordoban speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dataset structuring&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Traditional approach&lt;/strong&gt;: Use of a single high-quality speaker with a large amount of data (model trained for each speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Current and future approach&lt;/strong&gt;: Use of diverse datasets with labels that enable training models capable of understanding speech concepts and generalizing across multiple tasks (&lt;em&gt;VoiceBox&lt;/em&gt; examples).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;: Discussion of the increasing model size and training difficulty, similar to large-scale language models (&lt;em&gt;LLMs&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Explanation of Generative AI&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Introduction to generative models and the concept of underlying distribution.&lt;/li&gt;
&lt;li&gt;Explanation of &lt;em&gt;Transformers&lt;/em&gt; and their function in classification with context. Clarification that while &lt;em&gt;Transformers&lt;/em&gt; are &lt;em&gt;Non-Autoregressive&lt;/em&gt; (NAR) during training, they operate &lt;em&gt;Autoregressively&lt;/em&gt; (AR) in inference.&lt;/li&gt;
&lt;li&gt;Difference between classification models and generative models based on dimensionality reduction and latent variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tts-models-voicebox-e2-and-f5&#34;&gt;TTS Models: VoiceBox, E2, and F5
&lt;/h2&gt;&lt;p&gt;This section details the key advancements in TTS models leading to the development of &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;1-voicebox-tts---in-context-learning&#34;&gt;1. VoiceBox TTS - &lt;em&gt;In Context Learning&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;The initial architecture of &lt;em&gt;VoiceBox&lt;/em&gt; is introduced, focusing on generalization through &lt;em&gt;In Context Learning&lt;/em&gt;. This approach allows the model to infer information based on prior examples. To achieve this, it is given an audio sample with a missing segment, forcing it to complete it coherently.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;We show that Voicebox’s text-guided speech infilling approach is much more scalable in terms of data while subsuming many common speech generative tasks.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This method, introduced by Meta in &lt;em&gt;VoiceBox&lt;/em&gt;, represents a paradigm shift by demonstrating that TTS models can benefit from techniques previously used in language and image models.&lt;/p&gt;
&lt;p&gt;However, the model still relies on classical TTS techniques such as phoneme duration prediction and &lt;em&gt;Forced Alignment&lt;/em&gt; to map phonemes to Mel spectrograms, as their lengths do not match those of text tokens.&lt;/p&gt;
&lt;h3 id=&#34;2-e2-tts---filler-tokens&#34;&gt;2. E2 TTS - &lt;em&gt;Filler Tokens&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;E2 TTS&lt;/em&gt; simplifies many of the architectural decisions of &lt;em&gt;VoiceBox&lt;/em&gt;, particularly in handling the differing lengths of Mel spectrograms and text. Instead of using forced alignment, it introduces &lt;em&gt;Filler Tokens&lt;/em&gt;, which allow text and audio dimensions to be equalized more efficiently. This approach makes it easier for the model to learn temporal associations between the two elements.&lt;/p&gt;
&lt;p&gt;Another significant change is that the model directly processes text instead of phonemes. While this might seem disadvantageous (since homographic words can have different pronunciations), context provides enough information to infer the correct pronunciation, similar to how humans process natural language.&lt;/p&gt;
&lt;h3 id=&#34;3-f5-tts---open-source&#34;&gt;3. F5 TTS - &lt;em&gt;Open Source&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;Although &lt;em&gt;E2 TTS&lt;/em&gt; is a major step forward, its training is slow due to the need to infer associations that were previously guided. &lt;em&gt;F5 TTS&lt;/em&gt; optimizes the previous model with the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use of ConvNeXt&lt;/strong&gt;: Enhances text processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architectural change&lt;/strong&gt;: Replaces the &lt;em&gt;U-Net&lt;/em&gt; network with &lt;em&gt;DiT&lt;/em&gt; (&lt;em&gt;Diffusion Transformer&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sway Sampling&lt;/strong&gt;: Optimizes the inference process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, &lt;em&gt;F5-TTS&lt;/em&gt; is open-source, facilitating adoption and improvement by the community.&lt;/p&gt;
&lt;h2 id=&#34;flow-matching---theoretical-foundations&#34;&gt;Flow Matching - Theoretical Foundations
&lt;/h2&gt;&lt;p&gt;To understand how &lt;em&gt;F5-TTS&lt;/em&gt; works, it is essential to review the models it is based on, known as &lt;em&gt;Flow Matching Models&lt;/em&gt;. These models have evolved from previous techniques in probabilistic distribution modeling.&lt;/p&gt;
&lt;h3 id=&#34;1-normalizing-flow&#34;&gt;1. Normalizing Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Normalizing Flows&lt;/em&gt; are a family of probabilistic models that transform a simple distribution into a more complex one through a series of invertible and differentiable functions. They are used to model high-dimensional data distributions and generate realistic samples with precise control over probability.&lt;/p&gt;
&lt;h3 id=&#34;2-residual-flow&#34;&gt;2. Residual Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Residual Flows&lt;/em&gt; extend &lt;em&gt;Normalizing Flows&lt;/em&gt; by allowing more flexible transformations without strictly adhering to the invertibility condition. This is achieved through residual networks that approximate transformation functions without structural constraints.&lt;/p&gt;
&lt;h3 id=&#34;3-continuous-normalizing-flows-cnf&#34;&gt;3. Continuous Normalizing Flows (CNF)
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Continuous Normalizing Flows&lt;/em&gt; reformulate &lt;em&gt;Normalizing Flows&lt;/em&gt; using ordinary differential equations (ODEs). Instead of applying discrete transformations in successive steps, these models learn a continuous dynamic in the latent space, allowing for greater expressiveness and computational efficiency.&lt;/p&gt;
&lt;h3 id=&#34;4-flow-matching&#34;&gt;4. Flow Matching
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Flow Matching&lt;/em&gt; is a technique that optimizes the transformation of a reference distribution into a target distribution by minimizing a specific cost function. It differs from previous approaches by not requiring explicit density function calculations, making it particularly useful for large-scale generative models such as &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;online-presentation&#34;&gt;Online Presentation
&lt;/h2&gt;&lt;p&gt;All of this sections are explained in detail on the online class avaliable here (in Spanish):&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/YIpvFC41NUw&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>Understanding FastPitch and the Transformer Architecture</title>
        <link>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</link>
        <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</guid>
        <description>&lt;img src="http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/front.png" alt="Featured image of post Understanding FastPitch and the Transformer Architecture" /&gt;&lt;p&gt;Understanding a modern deep learning model is challenging due to the extensive prior knowledge required and the rapid pace of advancements in the field. In the research project &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we are working with TTS, specifically the FastPitch model from Nvidia. I have studied this model to fine-tune it for Spanish and shared my research process in a class to help my colleagues in the research group understand it better.&lt;/p&gt;
&lt;h2 id=&#34;understanding-seq2seq-models&#34;&gt;Understanding Seq2Seq Models
&lt;/h2&gt;&lt;p&gt;In &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we previously used Tacotron2 as our TTS model. While Tacotron2 performs well, it has several issues, primarily during training and inference, due to its auto-regressive nature. In contrast, FastPitch is a non-auto-regressive (NAR) model. To grasp these distinctions, I delved into seq2seq models, exploring their evolution over time and creating a quick overview of the sequence analysis models that have been pivotal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tacotron2 is based on an LSTM (AR) model, whereas FastPitch utilizes Transformers (NAR). Understanding this technological progression provides crucial background knowledge, particularly about transformers, including positional encoding, a key factor in their non-auto-regressive nature, and the attention mechanism.&lt;/p&gt;
&lt;h2 id=&#34;the-transformer-architecture&#34;&gt;The Transformer Architecture
&lt;/h2&gt;&lt;p&gt;I began by studying the transformer architecture, as it is fundamental to the FastPitch model. I reviewed online resources and the seminal &lt;em&gt;Attention is All You Need&lt;/em&gt; paper. Below are some key points I noted during my study:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Mechanism&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Dynamically focuses on different parts of the input sequence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Query (Q), Key (K), Value (V): Derived from input embeddings.&lt;/li&gt;
&lt;li&gt;Attention scores are computed as the dot product of Q and K, scaled by the square root of the dimension.&lt;/li&gt;
&lt;li&gt;Scores are normalized using softmax to create attention weights.&lt;/li&gt;
&lt;li&gt;A weighted sum of V is computed based on these weights to produce the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Captures diverse relationships between tokens by applying multiple self-attention mechanisms in parallel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Outputs from multiple attention heads are concatenated and linearly transformed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Provides information about token order in the sequence, compensating for the Transformer&amp;rsquo;s lack of built-in sequence order (unlike RNNs).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: A fixed or learnable vector is added to the input embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Encoder-Decoder Structure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Processes the input sequence into context-rich representations.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Multi-head self-attention&lt;/li&gt;
&lt;li&gt;Feed-forward neural network (FFN)&lt;/li&gt;
&lt;li&gt;Layer normalization and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Generates the output sequence by attending to both encoder outputs and previously generated tokens.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Masked multi-head self-attention (prevents attending to future tokens)&lt;/li&gt;
&lt;li&gt;Multi-head attention over encoder outputs&lt;/li&gt;
&lt;li&gt;FFN, layer normalization, and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feed-Forward Network (FFN)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Introduces non-linearity and processes each token independently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Two linear layers with a ReLU activation in between.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Layer Normalization and Residual Connections&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Stabilizes training and improves gradient flow by normalizing inputs to each layer and adding skip connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fastpitch&#34;&gt;FastPitch
&lt;/h2&gt;&lt;p&gt;After covering the theory, I examined each section of the FastPitch architecture in detail. I provided a brief explanation of word embeddings and positional encoding, as these are complex topics, and I wanted to keep the class concise.
FastPitch converts text into mel spectrograms, which are then transformed into audio by another model (in our case, HiFi-GAN). The training sequence involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Text to Word Embedding&lt;/li&gt;
&lt;li&gt;Word Embedding concatenated with mel spectrogram&lt;/li&gt;
&lt;li&gt;Positional encoding and FFT (Feed-Forward Transformer block)&lt;/li&gt;
&lt;li&gt;Pitch Prediction&lt;/li&gt;
&lt;li&gt;Phoneme Duration Prediction&lt;/li&gt;
&lt;li&gt;Another FFT block&lt;/li&gt;
&lt;li&gt;Fully connected layer&lt;/li&gt;
&lt;li&gt;Output mel spectrogram&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each block, I presented the corresponding equations and provided qualitative explanations for their roles in the model. For instance, phoneme duration prediction is crucial for aligning a phoneme&amp;rsquo;s duration with the expected duration in the spectrogram.&lt;/p&gt;
&lt;h2 id=&#34;online-class&#34;&gt;Online Class
&lt;/h2&gt;&lt;p&gt;Finally, I summarized the most important points and conducted an online class to share these concepts with my colleagues. You can watch it here (in Spanish):&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/v4bt8bGIM00&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        
    </channel>
</rss>
