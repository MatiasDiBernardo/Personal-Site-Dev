<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Math on Matias Di Bernardo</title>
        <link>http://localhost:1313/categories/math/</link>
        <description>Recent content in Math on Matias Di Bernardo</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Matías Di Bernardo</copyright>
        <lastBuildDate>Sun, 16 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/categories/math/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Generative AI, Flow Matching and TTS</title>
        <link>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</link>
        <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</guid>
        <description>&lt;img src="http://localhost:1313/p/generative-ai-flow-matching-and-tts/front.PNG" alt="Featured image of post Generative AI, Flow Matching and TTS" /&gt;&lt;p&gt;At &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we continuously work with state-of-the-art &lt;em&gt;Deep Learning&lt;/em&gt; models. To achieve this, we stay up to date with the latest developments in the field. In this presentation, the objective was to explain the fundamentals of the &lt;em&gt;F5-TTS&lt;/em&gt; model, a cutting-edge speech synthesis system. During the session, the concept of generative artificial intelligence and flow-based models was introduced, as &lt;em&gt;F5-TTS&lt;/em&gt; is based on this technique. The purpose is to share this knowledge with the team and with anyone interested in getting started with &lt;em&gt;Text-to-Speech&lt;/em&gt; (TTS) models.&lt;/p&gt;
&lt;h2 id=&#34;introduction---generative-ai-and-tts&#34;&gt;Introduction - Generative AI and TTS
&lt;/h2&gt;&lt;p&gt;The class was structured as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to TTS models&lt;/strong&gt;: Description of the different models used, their main characteristics, and the reasons behind changing approaches.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evolution of TTS models&lt;/strong&gt;: A graph-based survey illustrating the progression of various TTS technologies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paradigm shift in language models&lt;/strong&gt;: Explanation of how using large datasets has enabled better generalization in language models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example of generalization&lt;/strong&gt;: Illustrative cases, such as generating an image of an astronaut riding a horse, audio examples, and synthesis of specific accents (e.g., an angry Cordoban speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dataset structuring&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Traditional approach&lt;/strong&gt;: Use of a single high-quality speaker with a large amount of data (model trained for each speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Current and future approach&lt;/strong&gt;: Use of diverse datasets with labels that enable training models capable of understanding speech concepts and generalizing across multiple tasks (&lt;em&gt;VoiceBox&lt;/em&gt; examples).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;: Discussion of the increasing model size and training difficulty, similar to large-scale language models (&lt;em&gt;LLMs&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Explanation of Generative AI&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Introduction to generative models and the concept of underlying distribution.&lt;/li&gt;
&lt;li&gt;Explanation of &lt;em&gt;Transformers&lt;/em&gt; and their function in classification with context. Clarification that while &lt;em&gt;Transformers&lt;/em&gt; are &lt;em&gt;Non-Autoregressive&lt;/em&gt; (NAR) during training, they operate &lt;em&gt;Autoregressively&lt;/em&gt; (AR) in inference.&lt;/li&gt;
&lt;li&gt;Difference between classification models and generative models based on dimensionality reduction and latent variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tts-models-voicebox-e2-and-f5&#34;&gt;TTS Models: VoiceBox, E2, and F5
&lt;/h2&gt;&lt;p&gt;This section details the key advancements in TTS models leading to the development of &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;1-voicebox-tts---in-context-learning&#34;&gt;1. VoiceBox TTS - &lt;em&gt;In Context Learning&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;The initial architecture of &lt;em&gt;VoiceBox&lt;/em&gt; is introduced, focusing on generalization through &lt;em&gt;In Context Learning&lt;/em&gt;. This approach allows the model to infer information based on prior examples. To achieve this, it is given an audio sample with a missing segment, forcing it to complete it coherently.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;We show that Voicebox’s text-guided speech infilling approach is much more scalable in terms of data while subsuming many common speech generative tasks.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This method, introduced by Meta in &lt;em&gt;VoiceBox&lt;/em&gt;, represents a paradigm shift by demonstrating that TTS models can benefit from techniques previously used in language and image models.&lt;/p&gt;
&lt;p&gt;However, the model still relies on classical TTS techniques such as phoneme duration prediction and &lt;em&gt;Forced Alignment&lt;/em&gt; to map phonemes to Mel spectrograms, as their lengths do not match those of text tokens.&lt;/p&gt;
&lt;h3 id=&#34;2-e2-tts---filler-tokens&#34;&gt;2. E2 TTS - &lt;em&gt;Filler Tokens&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;E2 TTS&lt;/em&gt; simplifies many of the architectural decisions of &lt;em&gt;VoiceBox&lt;/em&gt;, particularly in handling the differing lengths of Mel spectrograms and text. Instead of using forced alignment, it introduces &lt;em&gt;Filler Tokens&lt;/em&gt;, which allow text and audio dimensions to be equalized more efficiently. This approach makes it easier for the model to learn temporal associations between the two elements.&lt;/p&gt;
&lt;p&gt;Another significant change is that the model directly processes text instead of phonemes. While this might seem disadvantageous (since homographic words can have different pronunciations), context provides enough information to infer the correct pronunciation, similar to how humans process natural language.&lt;/p&gt;
&lt;h3 id=&#34;3-f5-tts---open-source&#34;&gt;3. F5 TTS - &lt;em&gt;Open Source&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;Although &lt;em&gt;E2 TTS&lt;/em&gt; is a major step forward, its training is slow due to the need to infer associations that were previously guided. &lt;em&gt;F5 TTS&lt;/em&gt; optimizes the previous model with the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use of ConvNeXt&lt;/strong&gt;: Enhances text processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architectural change&lt;/strong&gt;: Replaces the &lt;em&gt;U-Net&lt;/em&gt; network with &lt;em&gt;DiT&lt;/em&gt; (&lt;em&gt;Diffusion Transformer&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sway Sampling&lt;/strong&gt;: Optimizes the inference process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, &lt;em&gt;F5-TTS&lt;/em&gt; is open-source, facilitating adoption and improvement by the community.&lt;/p&gt;
&lt;h2 id=&#34;flow-matching---theoretical-foundations&#34;&gt;Flow Matching - Theoretical Foundations
&lt;/h2&gt;&lt;p&gt;To understand how &lt;em&gt;F5-TTS&lt;/em&gt; works, it is essential to review the models it is based on, known as &lt;em&gt;Flow Matching Models&lt;/em&gt;. These models have evolved from previous techniques in probabilistic distribution modeling.&lt;/p&gt;
&lt;h3 id=&#34;1-normalizing-flow&#34;&gt;1. Normalizing Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Normalizing Flows&lt;/em&gt; are a family of probabilistic models that transform a simple distribution into a more complex one through a series of invertible and differentiable functions. They are used to model high-dimensional data distributions and generate realistic samples with precise control over probability.&lt;/p&gt;
&lt;h3 id=&#34;2-residual-flow&#34;&gt;2. Residual Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Residual Flows&lt;/em&gt; extend &lt;em&gt;Normalizing Flows&lt;/em&gt; by allowing more flexible transformations without strictly adhering to the invertibility condition. This is achieved through residual networks that approximate transformation functions without structural constraints.&lt;/p&gt;
&lt;h3 id=&#34;3-continuous-normalizing-flows-cnf&#34;&gt;3. Continuous Normalizing Flows (CNF)
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Continuous Normalizing Flows&lt;/em&gt; reformulate &lt;em&gt;Normalizing Flows&lt;/em&gt; using ordinary differential equations (ODEs). Instead of applying discrete transformations in successive steps, these models learn a continuous dynamic in the latent space, allowing for greater expressiveness and computational efficiency.&lt;/p&gt;
&lt;h3 id=&#34;4-flow-matching&#34;&gt;4. Flow Matching
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Flow Matching&lt;/em&gt; is a technique that optimizes the transformation of a reference distribution into a target distribution by minimizing a specific cost function. It differs from previous approaches by not requiring explicit density function calculations, making it particularly useful for large-scale generative models such as &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;online-presentation&#34;&gt;Online Presentation
&lt;/h2&gt;&lt;p&gt;All of this sections are explained on the online class avaliable here:&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/YIpvFC41NUw&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>Understanding FastPitch and the Transformer Architecture</title>
        <link>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</link>
        <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</guid>
        <description>&lt;img src="http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/front.png" alt="Featured image of post Understanding FastPitch and the Transformer Architecture" /&gt;&lt;p&gt;Understanding a modern deep learning model is challenging due to the extensive prior knowledge required and the rapid pace of advancements in the field. In the research project &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we are working with TTS, specifically the FastPitch model from Nvidia. I have studied this model to fine-tune it for Spanish and shared my research process in a class to help my colleagues in the research group understand it better.&lt;/p&gt;
&lt;h2 id=&#34;understanding-seq2seq-models&#34;&gt;Understanding Seq2Seq Models
&lt;/h2&gt;&lt;p&gt;In &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we previously used Tacotron2 as our TTS model. While Tacotron2 performs well, it has several issues, primarily during training and inference, due to its auto-regressive nature. In contrast, FastPitch is a non-auto-regressive (NAR) model. To grasp these distinctions, I delved into seq2seq models, exploring their evolution over time and creating a quick overview of the sequence analysis models that have been pivotal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tacotron2 is based on an LSTM (AR) model, whereas FastPitch utilizes Transformers (NAR). Understanding this technological progression provides crucial background knowledge, particularly about transformers, including positional encoding, a key factor in their non-auto-regressive nature, and the attention mechanism.&lt;/p&gt;
&lt;h2 id=&#34;the-transformer-architecture&#34;&gt;The Transformer Architecture
&lt;/h2&gt;&lt;p&gt;I began by studying the transformer architecture, as it is fundamental to the FastPitch model. I reviewed online resources and the seminal &lt;em&gt;Attention is All You Need&lt;/em&gt; paper. Below are some key points I noted during my study:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Mechanism&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Dynamically focuses on different parts of the input sequence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Query (Q), Key (K), Value (V): Derived from input embeddings.&lt;/li&gt;
&lt;li&gt;Attention scores are computed as the dot product of Q and K, scaled by the square root of the dimension.&lt;/li&gt;
&lt;li&gt;Scores are normalized using softmax to create attention weights.&lt;/li&gt;
&lt;li&gt;A weighted sum of V is computed based on these weights to produce the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Captures diverse relationships between tokens by applying multiple self-attention mechanisms in parallel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Outputs from multiple attention heads are concatenated and linearly transformed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Provides information about token order in the sequence, compensating for the Transformer&amp;rsquo;s lack of built-in sequence order (unlike RNNs).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: A fixed or learnable vector is added to the input embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Encoder-Decoder Structure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Processes the input sequence into context-rich representations.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Multi-head self-attention&lt;/li&gt;
&lt;li&gt;Feed-forward neural network (FFN)&lt;/li&gt;
&lt;li&gt;Layer normalization and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Generates the output sequence by attending to both encoder outputs and previously generated tokens.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Masked multi-head self-attention (prevents attending to future tokens)&lt;/li&gt;
&lt;li&gt;Multi-head attention over encoder outputs&lt;/li&gt;
&lt;li&gt;FFN, layer normalization, and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feed-Forward Network (FFN)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Introduces non-linearity and processes each token independently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Two linear layers with a ReLU activation in between.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Layer Normalization and Residual Connections&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Stabilizes training and improves gradient flow by normalizing inputs to each layer and adding skip connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fastpitch&#34;&gt;FastPitch
&lt;/h2&gt;&lt;p&gt;After covering the theory, I examined each section of the FastPitch architecture in detail. I provided a brief explanation of word embeddings and positional encoding, as these are complex topics, and I wanted to keep the class concise.
FastPitch converts text into mel spectrograms, which are then transformed into audio by another model (in our case, HiFi-GAN). The training sequence involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Text to Word Embedding&lt;/li&gt;
&lt;li&gt;Word Embedding concatenated with mel spectrogram&lt;/li&gt;
&lt;li&gt;Positional encoding and FFT (Feed-Forward Transformer block)&lt;/li&gt;
&lt;li&gt;Pitch Prediction&lt;/li&gt;
&lt;li&gt;Phoneme Duration Prediction&lt;/li&gt;
&lt;li&gt;Another FFT block&lt;/li&gt;
&lt;li&gt;Fully connected layer&lt;/li&gt;
&lt;li&gt;Output mel spectrogram&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each block, I presented the corresponding equations and provided qualitative explanations for their roles in the model. For instance, phoneme duration prediction is crucial for aligning a phoneme&amp;rsquo;s duration with the expected duration in the spectrogram.&lt;/p&gt;
&lt;h2 id=&#34;online-class&#34;&gt;Online Class
&lt;/h2&gt;&lt;p&gt;Finally, I summarized the most important points and conducted an online class to share these concepts with my colleagues. You can watch it here (in Spanish):&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/v4bt8bGIM00&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>Comparative analysis of time-frequency transformations</title>
        <link>http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/</link>
        <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/</guid>
        <description>&lt;img src="http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/fourier.jpg" alt="Featured image of post Comparative analysis of time-frequency transformations" /&gt;&lt;p&gt;This research is conducted as part of the subject &lt;em&gt;Metodología de la Investigación&lt;/em&gt; at UNTREF. The article aims to compare the differences between three types of time-frequency transformations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fourier Transform (FT)&lt;/li&gt;
&lt;li&gt;Wavelet Transform (WT)&lt;/li&gt;
&lt;li&gt;Huang-Hilbert Transform (HHT)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The objective of this work is to understand the differences between these types of transformations and deepen my knowledge of signal processing.&lt;/p&gt;
&lt;h2 id=&#34;objective&#34;&gt;Objective
&lt;/h2&gt;&lt;p&gt;The general objective of the research is to determine which spectral analysis tool achieves the highest accuracy in pitch detection tasks.&lt;/p&gt;
&lt;p&gt;To achieve this objective, the following specific objectives are proposed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify the parameters needed to represent the signal in the spectral domain for each case.&lt;/li&gt;
&lt;li&gt;Select an algorithm that identifies the pitch of the signal based on its spectral representation.&lt;/li&gt;
&lt;li&gt;Generate the data (audio signals) to be used for the comparison.&lt;/li&gt;
&lt;li&gt;Evaluate the generated data with the different analysis methods and apply statistical processes to validate the results.&lt;/li&gt;
&lt;li&gt;Establish a measure of accuracy for the pitch detection task.&lt;/li&gt;
&lt;li&gt;Compare the results of the different analyses and determine which method achieves the highest accuracy in pitch detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The pitch detection task was chosen because it is one of the main applications of these types of transformations.&lt;/p&gt;
&lt;h2 id=&#34;algorithms&#34;&gt;Algorithms
&lt;/h2&gt;&lt;p&gt;The theoretical analysis of all the transformations is performed in the continuous domain, but to conduct the experiments and comparisons, the discrete domain is used, enabling all calculations to be performed digitally.&lt;/p&gt;
&lt;h3 id=&#34;fft&#34;&gt;FFT
&lt;/h3&gt;&lt;p&gt;The FFT is an algorithm that optimizes the DFT (Discrete Fourier Transform). With this algorithm, the spectral representation of the signal is obtained according to Fourier analysis, which decomposes a complex signal into a sum of sines or cosines. The DFT is calculated using the formula:&lt;/p&gt;
$$
X_k = \sum_{n=0}^{N-1} e^{-i\frac{2\pi}{N}kn} x_n
$$&lt;p&gt;Where \( N \) is the number of signal samples, and \( k \) are natural numbers from \( 0 \) to \( N – 1 \).&lt;/p&gt;
&lt;h3 id=&#34;wt&#34;&gt;WT
&lt;/h3&gt;&lt;p&gt;The Wavelet Transform (WT) uses an oscillatory function (wavelet) and applies a convolution between the signal and the chosen wavelet function to determine whether that wave shape is present in the signal. The wavelet is stretched and scaled in frequency and amplitude, allowing a single wavelet function to recreate the entire spectrum of interest.&lt;/p&gt;
&lt;p&gt;In this research, the CDWT (Cyclic Discrete Wavelet Transform) will be used, the most common implementation when discretizing the WT. Conceptually, this transform extends Fourier analysis by projecting the signal onto a basis of wavelet functions instead of sines and cosines. It is calculated as follows:&lt;/p&gt;
$$
Wf[n, a^j] = \sum_{m=0}^{N-1} f[m] \psi_j[m-n]
$$&lt;p&gt;Where \( N \) is the number of signal samples, \( \psi \) is the wavelet function, and \( j \) represents the deformation of the wavelet according to the selected wavelet bank.&lt;/p&gt;
&lt;h3 id=&#34;hht&#34;&gt;HHT
&lt;/h3&gt;&lt;p&gt;Lastly, the Huang-Hilbert Transform (HHT) will be used for spectral representation. It employs a method called Empirical Mode Decomposition (EMD) to decompose the signal into subsignals that contain the relevant information of the original function.&lt;/p&gt;
&lt;p&gt;Like the previous analysis methods, the key part of the analysis is the decomposition of the signal into simpler signals. However, instead of sine or wavelet functions, EMD finds intrinsic mode functions (IMFs) that form the basis of our decomposition and are unique to each signal.&lt;/p&gt;
&lt;p&gt;The relationship between the IMFs and the frequency of the original signal is established with the equation:&lt;/p&gt;
$$
z(t) = f(t) + i H\{ f(t) \}
$$&lt;p&gt;Where \( f(t) \) is an IMF of the original signal, and \( H \) is the Hilbert Transform. This allows the IMF to be represented as a complex signal by projecting it onto the imaginary axis using the Hilbert Transform.&lt;/p&gt;
&lt;p&gt;Thus, the IMF is represented as a complex signal, and the amplitude and phase of each moment can be extracted to construct the spectral representation. Since a signal generally has multiple IMFs, this process is repeated for all of them, and the results are summed to obtain the complete spectrum.&lt;/p&gt;
&lt;h2 id=&#34;procedure&#34;&gt;Procedure
&lt;/h2&gt;&lt;p&gt;This research analyzes the relationship between types of spectral representation and accuracy in pitch detection.&lt;/p&gt;
&lt;p&gt;First, the parameters for the different transformations will be selected. Among the most critical parameters to determine is the number of samples for temporal windowing, as it determines the trade-off between temporal and frequency resolution.&lt;/p&gt;
&lt;p&gt;To ensure a fair comparison among the methods, data representing various cases of interest will be generated, including four types of signals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monophonic&lt;/strong&gt;: Signals with a single note corresponding to \( F_0 \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Polyphonic&lt;/strong&gt;: Signals with multiple notes where harmony determines \( F_0 \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Slow Transitions&lt;/strong&gt;: Signals with gradual changes in \( F_0 \).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fast Transitions&lt;/strong&gt;: Signals with abrupt changes in \( F_0 \).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The real value \( V(t) \) will be compared to the result \( P(t) \) from each transform, integrating the difference over time to calculate the accuracy.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results
&lt;/h2&gt;&lt;p&gt;At this stage, the task was to complete the research plan detailing the procedure and analysis methods. Dummy data was generated and statistically validated to simulate expected results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/res.PNG&#34;
	width=&#34;834&#34;
	height=&#34;550&#34;
	srcset=&#34;http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/res_hu8387233291062647437.PNG 480w, http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/res_hu2809804783985204141.PNG 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Graph showing the precision of each transform according to the type of signal analyzed&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;363px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The graph compares the precision achieved by the three transformations for different signal types. Based on the properties of the transforms, the Wavelet Transform (WT) is expected to outperform the Fourier Transform (FT), and the Huang-Hilbert Transform (HHT) is expected to achieve the highest precision overall.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions
&lt;/h2&gt;&lt;p&gt;In pitch detection tasks using spectral analysis, the Huang-Hilbert Transform (HHT) generally provides higher precision than the Fast Fourier Transform (FFT) and the Cyclic Discrete Wavelet Transform (CDWT).&lt;/p&gt;
&lt;p&gt;The significance of this precision gain depends on the type of signal being analyzed, with fast-transition signals benefiting the least from the transformation change, while polyphonic signals show the most significant improvement when using the HHT.&lt;/p&gt;
&lt;p&gt;This project allowed me to deepen my understanding of signal processing and grasp the foundations of why tools like the WT and HHT are used based on the characteristics of the signal being analyzed.&lt;/p&gt;
&lt;p&gt;All details of this work are available in the following &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/1G5kasP3BzZPVuxrXArHM72pUVlkN9b2Q/view?usp=sharing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;report&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
