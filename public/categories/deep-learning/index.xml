<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deep Learning on Matias Di Bernardo</title>
        <link>http://localhost:1313/categories/deep-learning/</link>
        <description>Recent content in Deep Learning on Matias Di Bernardo</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Matías Di Bernardo</copyright>
        <lastBuildDate>Sun, 16 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Generative AI, Flow Matching and TTS</title>
        <link>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</link>
        <pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/generative-ai-flow-matching-and-tts/</guid>
        <description>&lt;img src="http://localhost:1313/p/generative-ai-flow-matching-and-tts/front.PNG" alt="Featured image of post Generative AI, Flow Matching and TTS" /&gt;&lt;p&gt;At &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we continuously work with state-of-the-art &lt;em&gt;Deep Learning&lt;/em&gt; models. To achieve this, we stay up to date with the latest developments in the field. In this presentation, the objective was to explain the fundamentals of the &lt;em&gt;F5-TTS&lt;/em&gt; model, a cutting-edge speech synthesis system. During the session, the concept of generative artificial intelligence and flow-based models was introduced, as &lt;em&gt;F5-TTS&lt;/em&gt; is based on this technique. The purpose is to share this knowledge with the team and with anyone interested in getting started with &lt;em&gt;Text-to-Speech&lt;/em&gt; (TTS) models.&lt;/p&gt;
&lt;h2 id=&#34;introduction---generative-ai-and-tts&#34;&gt;Introduction - Generative AI and TTS
&lt;/h2&gt;&lt;p&gt;The class was structured as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to TTS models&lt;/strong&gt;: Description of the different models used, their main characteristics, and the reasons behind changing approaches.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evolution of TTS models&lt;/strong&gt;: A graph-based survey illustrating the progression of various TTS technologies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paradigm shift in language models&lt;/strong&gt;: Explanation of how using large datasets has enabled better generalization in language models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Example of generalization&lt;/strong&gt;: Illustrative cases, such as generating an image of an astronaut riding a horse, audio examples, and synthesis of specific accents (e.g., an angry Cordoban speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change in dataset structuring&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Traditional approach&lt;/strong&gt;: Use of a single high-quality speaker with a large amount of data (model trained for each speaker).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Current and future approach&lt;/strong&gt;: Use of diverse datasets with labels that enable training models capable of understanding speech concepts and generalizing across multiple tasks (&lt;em&gt;VoiceBox&lt;/em&gt; examples).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;: Discussion of the increasing model size and training difficulty, similar to large-scale language models (&lt;em&gt;LLMs&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Explanation of Generative AI&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Introduction to generative models and the concept of underlying distribution.&lt;/li&gt;
&lt;li&gt;Explanation of &lt;em&gt;Transformers&lt;/em&gt; and their function in classification with context. Clarification that while &lt;em&gt;Transformers&lt;/em&gt; are &lt;em&gt;Non-Autoregressive&lt;/em&gt; (NAR) during training, they operate &lt;em&gt;Autoregressively&lt;/em&gt; (AR) in inference.&lt;/li&gt;
&lt;li&gt;Difference between classification models and generative models based on dimensionality reduction and latent variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;tts-models-voicebox-e2-and-f5&#34;&gt;TTS Models: VoiceBox, E2, and F5
&lt;/h2&gt;&lt;p&gt;This section details the key advancements in TTS models leading to the development of &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;1-voicebox-tts---in-context-learning&#34;&gt;1. VoiceBox TTS - &lt;em&gt;In Context Learning&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;The initial architecture of &lt;em&gt;VoiceBox&lt;/em&gt; is introduced, focusing on generalization through &lt;em&gt;In Context Learning&lt;/em&gt;. This approach allows the model to infer information based on prior examples. To achieve this, it is given an audio sample with a missing segment, forcing it to complete it coherently.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;We show that Voicebox’s text-guided speech infilling approach is much more scalable in terms of data while subsuming many common speech generative tasks.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This method, introduced by Meta in &lt;em&gt;VoiceBox&lt;/em&gt;, represents a paradigm shift by demonstrating that TTS models can benefit from techniques previously used in language and image models.&lt;/p&gt;
&lt;p&gt;However, the model still relies on classical TTS techniques such as phoneme duration prediction and &lt;em&gt;Forced Alignment&lt;/em&gt; to map phonemes to Mel spectrograms, as their lengths do not match those of text tokens.&lt;/p&gt;
&lt;h3 id=&#34;2-e2-tts---filler-tokens&#34;&gt;2. E2 TTS - &lt;em&gt;Filler Tokens&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;E2 TTS&lt;/em&gt; simplifies many of the architectural decisions of &lt;em&gt;VoiceBox&lt;/em&gt;, particularly in handling the differing lengths of Mel spectrograms and text. Instead of using forced alignment, it introduces &lt;em&gt;Filler Tokens&lt;/em&gt;, which allow text and audio dimensions to be equalized more efficiently. This approach makes it easier for the model to learn temporal associations between the two elements.&lt;/p&gt;
&lt;p&gt;Another significant change is that the model directly processes text instead of phonemes. While this might seem disadvantageous (since homographic words can have different pronunciations), context provides enough information to infer the correct pronunciation, similar to how humans process natural language.&lt;/p&gt;
&lt;h3 id=&#34;3-f5-tts---open-source&#34;&gt;3. F5 TTS - &lt;em&gt;Open Source&lt;/em&gt;
&lt;/h3&gt;&lt;p&gt;Although &lt;em&gt;E2 TTS&lt;/em&gt; is a major step forward, its training is slow due to the need to infer associations that were previously guided. &lt;em&gt;F5 TTS&lt;/em&gt; optimizes the previous model with the following improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Use of ConvNeXt&lt;/strong&gt;: Enhances text processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architectural change&lt;/strong&gt;: Replaces the &lt;em&gt;U-Net&lt;/em&gt; network with &lt;em&gt;DiT&lt;/em&gt; (&lt;em&gt;Diffusion Transformer&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sway Sampling&lt;/strong&gt;: Optimizes the inference process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, &lt;em&gt;F5-TTS&lt;/em&gt; is open-source, facilitating adoption and improvement by the community.&lt;/p&gt;
&lt;h2 id=&#34;flow-matching---theoretical-foundations&#34;&gt;Flow Matching - Theoretical Foundations
&lt;/h2&gt;&lt;p&gt;To understand how &lt;em&gt;F5-TTS&lt;/em&gt; works, it is essential to review the models it is based on, known as &lt;em&gt;Flow Matching Models&lt;/em&gt;. These models have evolved from previous techniques in probabilistic distribution modeling.&lt;/p&gt;
&lt;h3 id=&#34;1-normalizing-flow&#34;&gt;1. Normalizing Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Normalizing Flows&lt;/em&gt; are a family of probabilistic models that transform a simple distribution into a more complex one through a series of invertible and differentiable functions. They are used to model high-dimensional data distributions and generate realistic samples with precise control over probability.&lt;/p&gt;
&lt;h3 id=&#34;2-residual-flow&#34;&gt;2. Residual Flow
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Residual Flows&lt;/em&gt; extend &lt;em&gt;Normalizing Flows&lt;/em&gt; by allowing more flexible transformations without strictly adhering to the invertibility condition. This is achieved through residual networks that approximate transformation functions without structural constraints.&lt;/p&gt;
&lt;h3 id=&#34;3-continuous-normalizing-flows-cnf&#34;&gt;3. Continuous Normalizing Flows (CNF)
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Continuous Normalizing Flows&lt;/em&gt; reformulate &lt;em&gt;Normalizing Flows&lt;/em&gt; using ordinary differential equations (ODEs). Instead of applying discrete transformations in successive steps, these models learn a continuous dynamic in the latent space, allowing for greater expressiveness and computational efficiency.&lt;/p&gt;
&lt;h3 id=&#34;4-flow-matching&#34;&gt;4. Flow Matching
&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Flow Matching&lt;/em&gt; is a technique that optimizes the transformation of a reference distribution into a target distribution by minimizing a specific cost function. It differs from previous approaches by not requiring explicit density function calculations, making it particularly useful for large-scale generative models such as &lt;em&gt;F5-TTS&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;online-presentation&#34;&gt;Online Presentation
&lt;/h2&gt;&lt;p&gt;All of this sections are explained on the online class avaliable here:&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/YIpvFC41NUw&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>The effect of denosing on TTS</title>
        <link>http://localhost:1313/p/the-effect-of-denosing-on-tts/</link>
        <pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/the-effect-of-denosing-on-tts/</guid>
        <description>&lt;p&gt;This study was conducted in the context of the class &lt;em&gt;Laboratorio de Acústica&lt;/em&gt; at UNTREF. I chose this topic because it aligns with research I have been pursuing as part of the group &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;. The class assignment involved conducting a subjective study using a survey to explore the relationship between objective and subjective variables.&lt;/p&gt;
&lt;p&gt;In my research group, I have been investigating how denoising algorithms affect Text-to-Speech (TTS) systems trained on low-quality recordings. The focus is on Rioplatense Spanish, a regional accent with limited high-quality data. Within this context, it was natural to combine both tasks and perform a subjective test on the impact of denoising algorithms on TTS systems.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview
&lt;/h2&gt;&lt;p&gt;The key points of this investigation are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of three denoising algorithms: Wave U-Net, HiFi-GAN, and DeepFilterNet.&lt;/li&gt;
&lt;li&gt;Use of both subjective (CMOS) and objective metrics (PESQ, STOI, MCD).&lt;/li&gt;
&lt;li&gt;Insights into resource-efficient TTS model development for underrepresented accents.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Algorithms&lt;/strong&gt;: Wave U-Net, HiFi-GAN, and DeepFilterNet evaluated with the FastPitch TTS model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: Subset of the ArchiVoz collection (15 minutes of noisy audio).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;: CMOS subjective test and objective metrics (PESQ, STOI, MCD).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participants&lt;/strong&gt;: 24 valid responses, including both experts and non-experts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-findings&#34;&gt;Key Findings
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepFilterNet Performance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Achieved the highest CMOS score, reflecting the best subjective quality.&lt;/li&gt;
&lt;li&gt;Demonstrated significant improvements in TTS output despite mixed correlations with objective metrics.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Objective Metrics Analysis&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PESQ and MCD showed limited correlation with subjective preferences.&lt;/li&gt;
&lt;li&gt;STOI scores were consistent across algorithms, indicating preserved intelligibility.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Algorithm Comparisons&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DeepFilterNet&lt;/strong&gt;: Superior subjective evaluations, moderate MCD.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demucs&lt;/strong&gt;: Comparable to DeepFilterNet in PESQ but lower subjective scores.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wave U-Net&lt;/strong&gt;: Poor subjective and objective performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Subject Expertise&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No significant differences were observed between expert and non-expert evaluations in subjective testing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;implications&#34;&gt;Implications
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Advanced denoising methods like DeepFilterNet can enhance TTS systems without requiring high-quality recordings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitations&lt;/strong&gt;: Objective metrics like PESQ and MCD are insufficient standalone indicators of subjective TTS quality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Future Work&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Expand datasets and noise levels for more robust analysis.&lt;/li&gt;
&lt;li&gt;Explore TTS systems trained jointly with denoising algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions
&lt;/h2&gt;&lt;p&gt;This work concludes that preprocessing with DeepFilterNet significantly improves TTS performance, with a 1.1 CMOS score increase. These findings underscore the importance of algorithm selection in optimizing low-resource TTS systems. Additionally, I gained valuable insights into subjective evaluations and the statistical analysis required to draw meaningful conclusions from data.&lt;/p&gt;
&lt;p&gt;All the information for this study can be found in the &lt;a class=&#34;link&#34; href=&#34;https://drive.google.com/file/d/1F4aJGIU9FX2LT8OFik-Yjg4uSz6T09jw/view?usp=sharing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;academic report&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Understanding FastPitch and the Transformer Architecture</title>
        <link>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</link>
        <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/</guid>
        <description>&lt;img src="http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/front.png" alt="Featured image of post Understanding FastPitch and the Transformer Architecture" /&gt;&lt;p&gt;Understanding a modern deep learning model is challenging due to the extensive prior knowledge required and the rapid pace of advancements in the field. In the research project &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we are working with TTS, specifically the FastPitch model from Nvidia. I have studied this model to fine-tune it for Spanish and shared my research process in a class to help my colleagues in the research group understand it better.&lt;/p&gt;
&lt;h2 id=&#34;understanding-seq2seq-models&#34;&gt;Understanding Seq2Seq Models
&lt;/h2&gt;&lt;p&gt;In &lt;em&gt;Intercambios Transorgánicos&lt;/em&gt;, we previously used Tacotron2 as our TTS model. While Tacotron2 performs well, it has several issues, primarily during training and inference, due to its auto-regressive nature. In contrast, FastPitch is a non-auto-regressive (NAR) model. To grasp these distinctions, I delved into seq2seq models, exploring their evolution over time and creating a quick overview of the sequence analysis models that have been pivotal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tacotron2 is based on an LSTM (AR) model, whereas FastPitch utilizes Transformers (NAR). Understanding this technological progression provides crucial background knowledge, particularly about transformers, including positional encoding, a key factor in their non-auto-regressive nature, and the attention mechanism.&lt;/p&gt;
&lt;h2 id=&#34;the-transformer-architecture&#34;&gt;The Transformer Architecture
&lt;/h2&gt;&lt;p&gt;I began by studying the transformer architecture, as it is fundamental to the FastPitch model. I reviewed online resources and the seminal &lt;em&gt;Attention is All You Need&lt;/em&gt; paper. Below are some key points I noted during my study:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Attention Mechanism&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Dynamically focuses on different parts of the input sequence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Query (Q), Key (K), Value (V): Derived from input embeddings.&lt;/li&gt;
&lt;li&gt;Attention scores are computed as the dot product of Q and K, scaled by the square root of the dimension.&lt;/li&gt;
&lt;li&gt;Scores are normalized using softmax to create attention weights.&lt;/li&gt;
&lt;li&gt;A weighted sum of V is computed based on these weights to produce the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Captures diverse relationships between tokens by applying multiple self-attention mechanisms in parallel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Outputs from multiple attention heads are concatenated and linearly transformed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Positional Encoding&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Provides information about token order in the sequence, compensating for the Transformer&amp;rsquo;s lack of built-in sequence order (unlike RNNs).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: A fixed or learnable vector is added to the input embeddings.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Encoder-Decoder Structure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Processes the input sequence into context-rich representations.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Multi-head self-attention&lt;/li&gt;
&lt;li&gt;Feed-forward neural network (FFN)&lt;/li&gt;
&lt;li&gt;Layer normalization and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Generates the output sequence by attending to both encoder outputs and previously generated tokens.
&lt;ul&gt;
&lt;li&gt;Components:
&lt;ul&gt;
&lt;li&gt;Masked multi-head self-attention (prevents attending to future tokens)&lt;/li&gt;
&lt;li&gt;Multi-head attention over encoder outputs&lt;/li&gt;
&lt;li&gt;FFN, layer normalization, and residual connections&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feed-Forward Network (FFN)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Introduces non-linearity and processes each token independently.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Two linear layers with a ReLU activation in between.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Layer Normalization and Residual Connections&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Stabilizes training and improves gradient flow by normalizing inputs to each layer and adding skip connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;fastpitch&#34;&gt;FastPitch
&lt;/h2&gt;&lt;p&gt;After covering the theory, I examined each section of the FastPitch architecture in detail. I provided a brief explanation of word embeddings and positional encoding, as these are complex topics, and I wanted to keep the class concise.
FastPitch converts text into mel spectrograms, which are then transformed into audio by another model (in our case, HiFi-GAN). The training sequence involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Text to Word Embedding&lt;/li&gt;
&lt;li&gt;Word Embedding concatenated with mel spectrogram&lt;/li&gt;
&lt;li&gt;Positional encoding and FFT (Feed-Forward Transformer block)&lt;/li&gt;
&lt;li&gt;Pitch Prediction&lt;/li&gt;
&lt;li&gt;Phoneme Duration Prediction&lt;/li&gt;
&lt;li&gt;Another FFT block&lt;/li&gt;
&lt;li&gt;Fully connected layer&lt;/li&gt;
&lt;li&gt;Output mel spectrogram&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each block, I presented the corresponding equations and provided qualitative explanations for their roles in the model. For instance, phoneme duration prediction is crucial for aligning a phoneme&amp;rsquo;s duration with the expected duration in the spectrogram.&lt;/p&gt;
&lt;h2 id=&#34;online-class&#34;&gt;Online Class
&lt;/h2&gt;&lt;p&gt;Finally, I summarized the most important points and conducted an online class to share these concepts with my colleagues. You can watch it here (in Spanish):&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34;&gt;
    &lt;iframe loading=&#34;lazy&#34; 
            src=&#34;https://www.youtube.com/embed/v4bt8bGIM00&#34; 
            allowfullscreen 
            title=&#34;YouTube Video&#34;
    &gt;
    &lt;/iframe&gt;
&lt;/div&gt;

</description>
        </item>
        <item>
        <title>AI learns to play 2048 game</title>
        <link>http://localhost:1313/p/ai-learns-to-play-2048-game/</link>
        <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/ai-learns-to-play-2048-game/</guid>
        <description>&lt;img src="http://localhost:1313/p/ai-learns-to-play-2048-game/game_img.jpg" alt="Featured image of post AI learns to play 2048 game" /&gt;&lt;p&gt;I used this project as an introduction to Reinforcement Learning. Having mostly worked with supervised and unsupervised learning, I wanted to start with a small, simple project to quickly grasp the main ideas and create something fun. I followed this &lt;a class=&#34;link&#34; href=&#34;https://youtu.be/L8ypSXwyBds&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YouTube video&lt;/a&gt; as a reference, which explains how to use Reinforcement Learning (RL) to train a model capable of playing the snake game. To make it more challenging, I applied the same network to the 2048 game.&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code
&lt;/h2&gt;&lt;p&gt;The code has three main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Game&lt;/strong&gt;: Implements the game logic and the graphical user interface (GUI).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;/strong&gt;: Controls the gameplay.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Model&lt;/strong&gt;: A neural network that learns how to play and guides the agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;game-state&#34;&gt;Game State
&lt;/h3&gt;&lt;p&gt;I modeled the game state as a 4x4 matrix representing the board. The actions in the game are represented by a vector, with the following possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Left&lt;/strong&gt;: [1, 0, 0, 0]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Right&lt;/strong&gt;: [0, 1, 0, 0]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Up&lt;/strong&gt;: [0, 0, 1, 0]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Down&lt;/strong&gt;: [0, 0, 0, 1]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reward&#34;&gt;Reward
&lt;/h3&gt;&lt;p&gt;The RL model works with rewards to quantify when the agent performs well or poorly. Initially, I defined the following rewards:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+10 points&lt;/strong&gt;: When the agent doubles the points. This is crucial because, in 2048, points increase exponentially.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;-10 points&lt;/strong&gt;: When the game is lost. This serves as a straightforward negative reward.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Initially, the model showed slow improvement. To address this, I added an additional reward:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;+2 points&lt;/strong&gt;: When points are increased. This reward incentivizes actions that maximize board clearance and progress.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-the-model-works&#34;&gt;How the Model Works
&lt;/h2&gt;&lt;p&gt;The model is a simple linear neural network. It takes the game state as input and predicts the best next action to maximize the reward.&lt;/p&gt;
&lt;p&gt;Rewards are managed using a technique called Q-Learning. A &lt;em&gt;Q Value&lt;/em&gt; represents the quality of a decision based on the loss function. The loss function is derived from the &lt;em&gt;Bellman Equation&lt;/em&gt;:&lt;/p&gt;
$$
NewQ(s, a) = Q(s, a) + \alpha [R(s, a) + \lambda \, \text{max}Q&#39;(s&#39;, a&#39;) - Q(s, a)]
$$&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q(s, a)$: The &lt;em&gt;Q Value&lt;/em&gt; for a specific state and action.&lt;/li&gt;
&lt;li&gt;$\alpha$: Learning rate.&lt;/li&gt;
&lt;li&gt;$R(s, a)$: Reward for a specific state and action.&lt;/li&gt;
&lt;li&gt;$\lambda$: Discount rate.&lt;/li&gt;
&lt;li&gt;$\text{max}Q&amp;rsquo;(s&amp;rsquo;, a&amp;rsquo;)$: Maximum expected future reward.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments-and-results&#34;&gt;Experiments and Results
&lt;/h2&gt;&lt;h3 id=&#34;random-test-as-a-baseline&#34;&gt;Random Test as a Baseline
&lt;/h3&gt;&lt;p&gt;To establish a baseline, I tested the average score achievable by taking random actions. After 1,000 iterations, the average score was &lt;strong&gt;170&lt;/strong&gt;, far below the 2048 points needed to win the game.&lt;/p&gt;
&lt;h3 id=&#34;initial-results&#34;&gt;Initial Results
&lt;/h3&gt;&lt;p&gt;My initial attempts were discouraging. The model performed worse than random movements. Here are some early results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Game 1,000&lt;/strong&gt; | Score: 208 | Record: 416 | Mean Score: 200 | Reward: 640&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game 1,000&lt;/strong&gt; | Score: 116 | Record: 346 | Mean Score: 161 | Reward: 310&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game 1,000&lt;/strong&gt; | Score: 112 | Record: 348 | Mean Score: 146 | Reward: 320&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In these attempts, the agent developed a suboptimal strategy of filling the board before increasing points.&lt;/p&gt;
&lt;h3 id=&#34;improvements&#34;&gt;Improvements
&lt;/h3&gt;&lt;p&gt;After experimenting with the reward parameters, I focused on the exploration phase. Initially, the &lt;em&gt;exploration games&lt;/em&gt; parameter, which randomly picks moves, was set to 25 games. Increasing this parameter allowed the agent to explore more strategies, leading to better results:&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Game 1,000&lt;/strong&gt; | Score: 478 | Record: 478 | Mean Score: 230&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game 1,000&lt;/strong&gt; | Score: 514 | Record: 964 | Mean Score: 382&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the model improved, I extended the training to more games:&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Game 3,796&lt;/strong&gt; | Score: 770 | Record: 1,366 | Mean Score: 469&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Game 4,852&lt;/strong&gt; | Score: 631 | Record: 1,462 | Mean Score: 483&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, with 200 exploration games and 5,000 training games, the results were as follows:&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;&lt;strong&gt;Game 5,000&lt;/strong&gt; | Score: 840 | Record: 1,678 | Mean Score: 512&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although the model didn&amp;rsquo;t beat the game, I was satisfied with the progress. I believe that with longer training (the final run lasted 4 hours) and additional network layers, it would be possible to win the game using this architecture.&lt;/p&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo
&lt;/h2&gt;&lt;p&gt;Here is a demo of the application, showcasing the agent&amp;rsquo;s learning process:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/1045495533&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I implemented keybindings to control the game&amp;rsquo;s speed, providing three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: For rapid training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Slow&lt;/strong&gt;: To observe and analyze the agent&amp;rsquo;s progress and mistakes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium&lt;/strong&gt;: Rarely used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The complete code for this project is available in this &lt;a class=&#34;link&#34; href=&#34;https://github.com/MatiasDiBernardo/RF_2048-game&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;repository&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Evaluation of different models for Speaker Diarization task</title>
        <link>http://localhost:1313/p/evaluation-of-different-models-for-speaker-diarization-task/</link>
        <pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/evaluation-of-different-models-for-speaker-diarization-task/</guid>
        <description>&lt;p&gt;This project started as the final assignment for a seminar class at UNTREF called &lt;em&gt;Seminario en Aplicaciones de Redes Neuronales en la recuperación de información musical&lt;/em&gt;. The objective was to use a Siamese Neural Network (SNN) in a different context than the one explored in class (music similarity detection).&lt;/p&gt;
&lt;p&gt;For the final project of this course, we developed an SNN model from scratch using the &lt;em&gt;Keras&lt;/em&gt; framework and the &lt;em&gt;SincNet&lt;/em&gt; architecture to reduce audio dimensionality, achieving good results. Later, to expand this project, I tried another approach by using &lt;em&gt;Wav2Vec&lt;/em&gt; for dimensionality reduction and re-implementing the entire project in the &lt;em&gt;PyTorch&lt;/em&gt; framework. This attempt, however, yielded suboptimal results, indicating that the dimensionality reduction using Wav2Vec lost critical information required for the speaker diarization task.&lt;/p&gt;
&lt;h2 id=&#34;speaker-diarization-task&#34;&gt;Speaker Diarization Task
&lt;/h2&gt;&lt;p&gt;The goal of a speaker diarization model is to identify different speakers in an audio stream containing multiple speakers. For example, in a podcast with two people (A and B), the model must determine the time steps where speaker A is talking and the time steps where speaker B is speaking (and implicitly identify the periods of silence). These models are incredibly useful for audio editing and analyzing long audio sequences with multiple speakers.&lt;/p&gt;
&lt;h2 id=&#34;why-siamese-neural-networks&#34;&gt;Why Siamese Neural Networks?
&lt;/h2&gt;&lt;p&gt;In class, we explored the Siamese architecture to compare similarities between pieces of music, developing a tool capable of identifying covers of famous songs.&lt;/p&gt;
&lt;p&gt;A Siamese Neural Network consists of two or more identical subnetworks sharing the same weights and parameters. It is designed to compare input pairs and measure their similarity, typically using a distance metric like Euclidean distance. Each subnetwork processes one input, and the outputs are combined to compute a similarity score.&lt;/p&gt;
&lt;p&gt;With this similarity comparison in mind, we wanted to apply these networks to the speaker diarization task. The idea was to generate speaker embeddings from audio using a pre-trained model and compare the outputs of different audio segments. Based on the similarity score, we aimed to identify the segments where different speakers are talking.&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;p&gt;I tested two different methods for audio feature extraction to serve as speaker embeddings.&lt;/p&gt;
&lt;h3 id=&#34;keras-implementation-with-sincnet&#34;&gt;Keras Implementation with SincNet
&lt;/h3&gt;&lt;p&gt;In this approach, we used the SincNet architecture to extract speaker-specific features from audio. SincNet applies learnable sinc functions as its filters, which are particularly well-suited for audio processing as they mimic traditional bandpass filters. These features were then fed into the Siamese Neural Network, which compared pairs of audio segments to calculate their similarity scores. The model was trained on labeled audio datasets, and we observed strong performance in clustering audio segments by speaker, achieving clear boundaries between different speakers.&lt;/p&gt;
&lt;p&gt;A report with the results can be found in the following &lt;a class=&#34;link&#34; href=&#34;https://github.com/MatiasDiBernardo/Speaker-Diarization-with-SNN/blob/master/TP%20Final%20Seminario%20Redes%20-%20Di%20Bernardo%20Ferreyra.ipynb&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jupyter notebook&lt;/a&gt; (in Spanish).&lt;/p&gt;
&lt;h3 id=&#34;pytorch-implementation-with-wav2vec&#34;&gt;PyTorch Implementation with Wav2Vec
&lt;/h3&gt;&lt;p&gt;For this method, I utilized Wav2Vec, a powerful pre-trained model for extracting deep audio embeddings. Unlike SincNet, Wav2Vec embeddings are derived from self-supervised learning, capturing high-level representations of audio. These embeddings were used in the Siamese Neural Network for similarity comparisons. However, the results were suboptimal for the diarization task. It appears that Wav2Vec, while excellent for speech recognition tasks, lost some speaker-specific details necessary for distinguishing between speakers in our setup.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results
&lt;/h2&gt;&lt;p&gt;The experiments demonstrated that the choice of feature extraction method is crucial for speaker diarization. The Keras implementation with SincNet outperformed the PyTorch implementation with Wav2Vec, showing higher accuracy in identifying speaker transitions. This suggests that task-specific feature extraction, like SincNet, is more effective than general-purpose embeddings like Wav2Vec for speaker diarization.&lt;/p&gt;
&lt;p&gt;The code for this project is available in this &lt;a class=&#34;link&#34; href=&#34;https://github.com/MatiasDiBernardo/Speaker-Diarization-with-SNN&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions
&lt;/h2&gt;&lt;p&gt;This project was one of my first experiences with deep learning models, where I applied my knowledge to a problem without following a specific paper or using a pre-trained model. I explored different solutions and concluded on the importance of feature extraction and model selection.&lt;/p&gt;
&lt;p&gt;It also helped me become familiar with the syntax of the most popular deep learning frameworks and solidified my understanding in the process.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
