[{"content":"Este trabajo se desarrollo en el contexto de la materia Instrumentos y Mediciones Acústicas en UNTREF Argentina.\nIntroducción Las salas de control y los entornos de escucha crítica a menudo sufren de respuestas acústicas irregulares en bajas frecuencias. Estas irregularidades, causadas por una distribución dispersa de los modos de vibración, generan \u0026ldquo;coloraciones\u0026rdquo; que dificultan una evaluación sonora precisa. Tradicionalmente, se han utilizado criterios como los de Bonello, Bolt y Louden para optimizar la geometría de salas rectangulares y mejorar la distribución modal. Sin embargo, estas metodologías no consideran la influencia de contornos complejos ni la ubicación de la fuente y el oyente.\nEste trabajo presenta una herramienta de código abierto desarrollada en Python/FEniCS que aborda estas limitaciones. El software utiliza la optimización geométrica por fuerza bruta sobre modelos de elementos finitos (FEM) para encontrar dimensiones y contornos de sala que proporcionen una distribución modal más uniforme.\nMarco Teórico y Criterios Clásicos El comportamiento de las bajas frecuencias en un recinto está dominado por las ondas estacionarias, o modos normales, que se caracterizan por nodos y antinodos de presión. Los modos axiales, tangenciales y oblicuos, cuyas frecuencias dependen de las dimensiones de la sala, pueden generar problemas de coloración si se agrupan.\nLos criterios clásicos de diseño, como los de Bolt, Bonello y Louden, se centran en evitar agrupamientos modales y proponen proporciones geométricas óptimas para salas rectangulares. Sin embargo, estos enfoques tienen una limitación importante: no consideran factores cruciales como la posición de la fuente sonora y el receptor, y están restringidos a geometrías simples.\nMetodología y Desarrollo del Software La herramienta desarrollada combina un proceso de optimización en dos etapas.\nBúsqueda Inicial: Primero, el software realiza una búsqueda rápida en geometrías de paralelepípedos rectangulares utilizando el método clásico de superposición modal (SM) para identificar las proporciones geométricas iniciales más prometedoras. Refinamiento y Optimización: Luego, refina la búsqueda generando contornos planarios aleatorios con simetría forzada y aplica el Método de Elementos Finitos en el dominio de la Frecuencia (FD-FEM) para evaluar el mérito acústico de geometrías complejas. Este método es más preciso que la superposición modal para geometrías no rectangulares. Para cuantificar el rendimiento acústico, se utiliza una figura de mérito combinada: la Desviación Media del Campo Sonoro (MSFD). Esta métrica integra dos parámetros clave:\nDesviación de Magnitud (MD): Mide cuán plana es la respuesta en frecuencia en una posición específica. Desviación Espacial (SD): Mide la variación de la magnitud a lo largo del área de escucha. La herramienta cuenta con una interfaz gráfica (GUI) en PyQt5 que permite al usuario definir las dimensiones, márgenes y posiciones de fuente y receptor, y visualizar los resultados y las geometrías optimizadas.\nResultados y Conclusiones Los estudios de caso en tres volúmenes de salas de control de referencia mostraron mejoras de MSFD de hasta 5 dB en comparación con el diseño base. Los resultados demuestran que:\nImpacto de los Márgenes: A medida que se aumenta el espacio disponible para la optimización, se obtienen mejores resultados, mejorando la respuesta general en hasta 3 dB. Esta mejora se observa principalmente en la desviación de magnitud (MD). Geometrías Complejas: El aumento de la cantidad de paredes en la geometría compleja permite obtener soluciones superiores a las de los paralelepípedos simples, con una diferencia media de 1.3 dB en el factor de mérito. El proceso de optimización no produce una única solución, sino una variedad de geometrías que presentan un mínimo de MSFD. Comparación con Criterios Tradicionales: Una geometría optimizada compleja superó a las salas con dimensiones basadas en los criterios clásicos de Bolt, Louden y Cox. Aunque estos criterios son efectivos y de costo computacional nulo, la capacidad del software para modelar geometrías complejas y considerar la ubicación de fuentes y receptores provee una condición superior. El estudio concluye que el software es una herramienta efectiva para la optimización de la acústica modal. Se sugieren mejoras futuras como la implementación de algoritmos de optimización más avanzados, como algoritmos genéticos, para reducir el tiempo de cálculo y mejorar la eficiencia del proceso.\nUn análisis detallado del desarrollo de este algoritmo está disponible en el siguiente trabajo.\n","date":"2025-06-26T00:00:00Z","image":"http://localhost:1313/p/acoustic-modal-response-optimization-for-small-rooms-with-genetic-algorithms/port_hu13224883107515297403.PNG","permalink":"http://localhost:1313/es/p/algoritmo-de-optimizaci%C3%B3n-modal-para-recintos-peque%C3%B1os/","title":"Algoritmo de optimización modal para recintos pequeños"},{"content":"Introducción Esta medición fue parte de la materia Instrumentos y mediciónes Acústica del programa de Ingeniería de Sonido en UNTREF. Los parámetros acústicos obtenidos a partir de la respuesta al impulso son esenciales para evaluar el comportamiento de un recinto. Este informe presenta una caracterización integral del auditorio principal de la Usina del Arte, un centro cultural en Buenos Aires. El edificio, originalmente una planta generadora de energía del siglo XX con un distintivo estilo florentino-industrial, fue transformado con un diseño acústico que buscaba una calidad natural y equilibrada, sin necesidad de amplificación. Se implementó una estructura desacoplada (box-in-box) para el aislamiento y un acondicionamiento interior con materiales como madera de guatambú, superficies difusoras y un reflector acústico suspendido. El objetivo era un tiempo de reverberación de aproximadamente 2 segundos y una distribución uniforme de las reflexiones tempranas laterales para una sensación envolvente.\nMedición La caracterización se llevó a cabo el 9 de junio de 2025, donde se registraron un total de 162 respuestas al impulso monoaurales y binaurales. Se registraron datos desde 27 posiciones de micrófono y 3 de fuente. También se realizó un relevamiento del auditorio para analizar sus características constructivas y un análisis perceptivo.\nPrevio a las mediciones, se construyó un modelo del recinto en el software EASE 4.3, que estimó un volumen de 15,700 m³ y una frecuencia de Schroeder de 22.1 Hz. El ruido de fondo se midió en ocho posiciones para evaluar el aislamiento, confirmando una relación señal/ruido superior a 40 dB. La disposición de los micrófonos se basó en la simetría del recinto para obtener un mapeo detallado.\nMas imágenes del proceso de la medición:\n\u0026lsaquo;\r\u0026rsaquo;\rProcesamiento Se procesaron las grabaciones para obtener las respuestas al impulso y se calcularon diversos parámetros, siguiendo la norma ISO 3382-1:\nTiempo de reverberación: $T_{20}$, $T_{30}$ y EDT. Claridad: $C_{50}$ y $C_{80}$. Fuerza (G): Diferencia de nivel de presión sonora entre la sala y una condición anecoica. Fracción lateral (LF): Proporción de energía sonora percibida desde los laterales. Relación directo/reverberante (D/R). Inteligibilidad: Se calculó el Speech Transmission Index (STI) y el Articulation Loss of Consonants (%Alcons). Soporte de escenario: $ST_{Early}$ y $ST_{Late}$, para evaluar las condiciones acústicas para los músicos. Se utilizaron diferentes software comericales como el plugin Aurora Acoustical Parameters, el software EASERA, y también se calcularon otros parámetros con un scripts específicos en Python.\nResultados Los resultados muestran que el auditorio se comporta adecuadamente para una sala de conciertos, pero con áreas de mejora:\nTiempo de reverberación: El promedio global fue de 1.92 s. Sin embargo, se observaron variaciones notables en bajas frecuencias, donde el escenario flotante actúa como un resonador. Claridad e Inteligibilidad: Los valores de claridad para el habla están por debajo de los umbrales recomendados, y se identificaron problemas de inteligibilidad en ciertas zonas.\nFactor de Nivel Sonoro (G): El nivel de fuerza sonora presenta una variación baja considerando las dimensiones del auditorio.\nFracción Lateral (LF): Los valores superan las recomendaciones, sugiriendo que la mayor parte del sonido proviene de los laterales. Esto podría estar relacionado con la gran cantidad de difusores.\nRuido de Fondo: La sala presenta un nivel de ruido superior al recomendado para un recinto sinfónico (NC-35 vs. NC-20), probablemente debido al sistema de ventilación.\nDifusión Sonora: La repetición de una única secuencia de difusores disminuye su eficacia, generando un comportamiento lobular en lugar de una difusión estocástica. Se proponen mejoras clave como reducir el ruido de fondo, optimizar la difusión sonora con secuencias no periódicas y equilibrar la respuesta espectral, corrigiendo la absorción en bajas frecuencias.\nConclusiones Pudimos caracterizar de forma efectiva el auditorio y se aplicaron la mayoría de temas teóricos vistos en clase a una experiencia práctica. El informe completo de este trabajo con todos los resultados y detalles de la medición se puede encontrar en el siguiente informe.\n","date":"2025-05-09T00:00:00Z","image":"http://localhost:1313/p/acoustic-measurement-at-usina-del-arte/portadix_hu1924722266797220890.jpg","permalink":"http://localhost:1313/es/p/medici%C3%B3n-de-par%C3%A1metros-ac%C3%BAsticos-en-la-usina-del-arte/","title":"Medición de parámetros acústicos en la Usina del Arte"},{"content":"En Intercambios Transorgánicos, trabajamos continuamente con modelos de Deep Learning de última generación. Para ello, nos mantenemos en constante aprendizaje sobre los desarrollos más recientes en el campo. En esta presentación, el objetivo fue explicar los fundamentos del modelo F5-TTS, un sistema de síntesis de voz de vanguardia. Durante el desarrollo, se introdujo el concepto de inteligencia artificial generativa y los modelos de flujo (flow-based models), dado que F5-TTS se basa en esta técnica. El propósito es compartir este conocimiento con el equipo y con cualquier persona interesada en iniciarse en los modelos de conversión de texto a voz (Text-to-Speech).\nIntroducción - IA Generativa y TTS La estructura de la clase se organizó de la siguiente manera:\nIntroducción a los modelos de TTS: Se describen los diferentes modelos utilizados, sus principales características y las razones por las cuales se decidió cambiar de enfoque. Evolución de los modelos TTS: Se presenta un gráfico basado en encuestas que muestra la evolución de diversas tecnologías de TTS. Cambio de paradigma en modelos de lenguaje: Se explica cómo el uso de grandes volúmenes de datos ha permitido una mejor generalización en modelos de lenguaje. Ejemplo de generalización: Se presentan casos ilustrativos, como la generación de una imagen de un astronauta montando un caballo, ejemplos de audio y síntesis de acentos específicos (ej., un hablante cordobés enojado). Cambio en la estructuración de conjuntos de datos: Enfoque tradicional: Uso de un único hablante de alta calidad con gran cantidad de datos (modelo entrenado para cada hablante). Enfoque actual y futuro: Uso de conjuntos de datos diversos con etiquetas que permiten entrenar modelos capaces de entender el concepto del habla y generalizar en múltiples tareas (ejemplos extraídos de VoiceBox). Costo computacional: Se menciona el aumento en el tamaño de los modelos y la dificultad de su entrenamiento, similar a los modelos de lenguaje a gran escala (LLMs). Explicación de IA Generativa: Introducción a los modelos generativos y el concepto de distribución subyacente. Explicación sobre Transformers y su función en la clasificación con contexto. Se aclara que aunque los Transformers son modelos Non-Autoregressive (NAR) en entrenamiento, en inferencia funcionan de manera Autoregressive (AR). Diferencia entre modelos de clasificación y modelos generativos basados en reducción de dimensionalidad y variables latentes. Modelos de TTS: VoiceBox, E2 y F5 Esta sección detalla los principales avances en modelos de TTS que han llevado al desarrollo de F5-TTS.\n1. VoiceBox TTS - In Context Learning Se introduce la arquitectura inicial de VoiceBox, cuyo objetivo principal es la generalización mediante In Context Learning. Este enfoque permite al modelo inferir información a partir de ejemplos previos. Para lograrlo, se le proporciona un fragmento de audio con una parte oculta, forzando al modelo a completarla de manera coherente.\n\u0026ldquo;We show that Voicebox’s text-guided speech infilling approach is much more scalable in terms of data while subsuming many common speech generative tasks.\u0026rdquo;\nEste método, introducido por Meta en VoiceBox, marca un cambio de paradigma al demostrar que los modelos de TTS pueden beneficiarse de técnicas previamente utilizadas en modelos de lenguaje e imagen.\nNo obstante, el modelo aún depende de técnicas clásicas en TTS, como el predictor de duración de fonemas y el Forced Alignment para mapear fonemas a espectrogramas de Mel, ya que la longitud de estos no coincide con la de los tokens de texto.\n2. E2 TTS - Filler Tokens E2 TTS simplifica varias de las decisiones arquitectónicas de VoiceBox, especialmente en lo referente a la longitud de los espectrogramas de Mel y el texto. En lugar de utilizar alineación forzada, introduce los Filler Tokens, que permiten igualar la dimensión entre texto y audio de manera más sencilla. Esta estrategia facilita que el modelo aprenda la asociación temporal entre ambos elementos.\nOtro cambio significativo es que el modelo trabaja directamente con texto en lugar de fonemas. Aunque esto podría parecer una desventaja (dado que palabras homógrafas pueden tener pronunciaciones diferentes), el contexto provee suficiente información para inferir la pronunciación adecuada, de manera similar al procesamiento del lenguaje natural en humanos.\n3. F5 TTS - Open Source Si bien E2 TTS representa un avance importante, su entrenamiento es lento debido a la necesidad de inferir asociaciones previamente guiadas. F5 TTS optimiza el modelo anterior con las siguientes mejoras:\nUso de ConvNeXt: Mejora el procesamiento del texto de entrada. Cambio de arquitectura: Sustitución de la red U-Net por DiT (Diffusion Transformer). Sway Sampling: Optimización en el proceso de inferencia. Además, F5 TTS es de código abierto, lo que facilita su adopción y mejora por parte de la comunidad.\nFlow Matching - Fundamentos Teóricos Para comprender el funcionamiento de F5-TTS, es necesario revisar los modelos en los que se basa, conocidos como Flow Matching Models. Estos modelos han evolucionado a partir de técnicas previas en modelado de distribuciones probabilísticas.\n1. Normalizing Flow Los Normalizing Flows son una familia de modelos probabilísticos que transforman una distribución simple en una más compleja mediante una serie de funciones invertibles y diferenciables. Se utilizan para modelar distribuciones de datos de alta dimensión y generar muestras realistas con un control preciso sobre la probabilidad.\n2. Residual Flow Los Residual Flows extienden los Normalizing Flows al permitir transformaciones más flexibles sin necesidad de cumplir estrictamente con la condición de invertibilidad. Esto se logra mediante redes residuales que aproximan las funciones de transformación sin restricciones de estructura.\n3. Continuous Normalizing Flows (CNF) Los Continuous Normalizing Flows reformulan los Normalizing Flows en términos de ecuaciones diferenciales ordinarias (ODEs). En lugar de aplicar transformaciones discretas en pasos sucesivos, estos modelos aprenden una dinámica continua en el espacio latente, lo que permite una mayor expresividad y eficiencia computacional.\n4. Flow Matching Flow Matching es una técnica que optimiza la transformación de una distribución de referencia a una distribución objetivo utilizando un enfoque basado en la minimización de una función de costo específica. Se diferencia de los enfoques anteriores al no requerir el cálculo explícito de la función de densidad, lo que lo hace especialmente útil para modelos generativos de gran escala, como F5-TTS.\nGrabación de la presentación Cada sección detalla en este artículo se explica en profundidad en la presentación:\n","date":"2025-03-24T00:00:00Z","image":"http://localhost:1313/p/generative-ai-flow-matching-and-tts/front2_hu7705949593533117439.PNG","permalink":"http://localhost:1313/es/p/ia-generativa-flow-matching-y-tts/","title":"IA Generativa, Flow Matching y TTS"},{"content":"Medidor THD+N Un medidor de THD+N mide la distorsión armónica de un dispositivo. En audio, esto es crucial para evaluar la calidad del equipo. Este proyecto fue desarrollado como parte de la asignatura \u0026ldquo;Instrumentos y Mediciones Electrónicas\u0026rdquo; en UNTREF. En este curso, los estudiantes diseñan y desarrollan diversos instrumentos electrónicos, y los proyectos son continuados por grupos sucesivos de estudiantes. Este proyecto en particular ya estaba en marcha, y nuestro enfoque fue desarrollar un desfasador para alinear dos señales.\nTeoría sobre como medición de THD Para medir la distorsión de un sistema, se introduce una señal sinusoidal con la menor distorsión posible al dispositivo bajo prueba. El objetivo es evaluar cuánta distorsión agrega el dispositivo a la señal. Esto se cuantifica midiendo la potencia de la señal original y la potencia de la señal después de pasar por el dispositivo, excluyendo la armónica fundamental.\nUn amplificador diferencial se utiliza para restar la señal de salida del dispositivo de la señal de referencia, dejando solo las armónicas de orden superior.\nEl THD+N se calcula luego como un porcentaje usando la siguiente ecuación:\n$$\rTHD+N = \\frac{V_{filt}}{V_{tot}} \\cdot 100\r$$Donde: $V_{filt}$ es el valor RMS de la señal filtrada (excluyendo la armónica fundamental). $V_{tot}$ es el valor RMS de la señal original.\nDesfasador Para lograr una cancelación efectiva en la sección diferencial, se requieren ajustes precisos de fase y ganancia de las dos señales. El ajuste de ganancia había sido implementado con éxito por el grupo anterior que trabajó en el proyecto. Sin embargo, lograr la rotación de fase necesaria de 360 grados en toda la gama de frecuencias audibles (20 Hz a 20 kHz) presentó un desafío.\nPara abordar este problema, diseñamos dos filtros pasa-todo en serie y calculamos los valores de los componentes para lograr la rotación de fase deseada en el rango de frecuencias objetivo.\nEl diseño fue modular para facilitar la integración con las etapas anteriores.\nProbamos el circuito en un protoboard antes de diseñar la PCB utilizando Altium Designer.\nEl dispositivo El dispositivo cuenta con varios controles para ajustar la fase y la ganancia. Tanto los ajustes de fase como de ganancia incluyen potenciómetros de ajuste fino para garantizar la máxima precisión.\n\u0026lsaquo;\r\u0026rsaquo;\rEstá equipado con entradas y salidas BNC, lo que permite a los usuarios visualizar la salida en un osciloscopio y lograr la máxima atenuación.\nResultados Este dispositivo se comparó con un medidor de THD comercial (GW INSTEK GAD-201G), y los resultados fueron muy similares. La principal limitación fue el nivel de ruido base del entorno de medición, que restringió significativamente el valor más bajo de THD que pudimos medir.\nLas especificaciones del dispositivo se resumen en la siguiente tabla (en español):\nUn análisis detallado del dispositivo está disponible en este informe.\n","date":"2024-11-26T00:00:00Z","image":"http://localhost:1313/p/thd-n-meter/port_hu1245660591834723805.jpeg","permalink":"http://localhost:1313/es/p/medidor-de-thd-n/","title":"Medidor de THD+N"},{"content":"Este estudio se realizó en el contexto de la clase Laboratorio de Acústica en la UNTREF. Elegí este tema porque está alineado con investigaciones que he estado desarrollando como parte del grupo Intercambios Transorgánicos. La consigna para el trabajo de clase consistió en realizar un estudio subjetivo utilizando una encuesta para explorar la relación entre variables objetivas y subjetivas.\nEn mi grupo de investigación, he estado investigando cómo los algoritmos de reducción de ruido afectan a los sistemas de síntesis de voz (TTS) entrenados con grabaciones de baja calidad. El enfoque está en el español rioplatense, un acento regional con datos de alta calidad limitados. En este contexto, fue natural combinar ambas tareas y realizar una prueba subjetiva sobre el impacto de los algoritmos de reducción de ruido en los sistemas TTS.\nResumen Los puntos clave de esta investigación son:\nEvaluación de tres algoritmos de reducción de ruido: Wave U-Net, HiFi-GAN y DeepFilterNet. Uso de métricas subjetivas (CMOS) y objetivas (PESQ, STOI, MCD). Ideas sobre el desarrollo de modelos TTS eficientes en recursos para acentos poco representados. Metodología Algoritmos: Wave U-Net, HiFi-GAN y DeepFilterNet evaluados con el modelo TTS FastPitch. Conjunto de datos: Subconjunto de la colección ArchiVoz (15 minutos de audio con ruido). Pruebas: Prueba subjetiva CMOS y métricas objetivas (PESQ, STOI, MCD). Participantes: 24 respuestas válidas, incluyendo tanto expertos como no expertos. Principales Hallazgos Rendimiento de DeepFilterNet:\nObtuvo la puntuación CMOS más alta, reflejando la mejor calidad subjetiva. Mostró mejoras significativas en la salida TTS a pesar de las correlaciones mixtas con las métricas objetivas. Análisis de Métricas Objetivas:\nPESQ y MCD mostraron una correlación limitada con las preferencias subjetivas. Las puntuaciones STOI fueron consistentes entre los algoritmos, indicando inteligibilidad preservada. Comparación de Algoritmos:\nDeepFilterNet: Evaluaciones subjetivas superiores, MCD moderado. Demucs: Comparable a DeepFilterNet en PESQ, pero con puntuaciones subjetivas inferiores. Wave U-Net: Bajo rendimiento tanto subjetivo como objetivo. Experiencia de los Participantes:\nNo se observaron diferencias significativas entre las evaluaciones subjetivas de expertos y no expertos. Implicaciones Eficiencia: Métodos avanzados de reducción de ruido como DeepFilterNet pueden mejorar los sistemas TTS sin necesidad de grabaciones de alta calidad. Limitaciones: Las métricas objetivas como PESQ y MCD no son indicadores suficientes por sí solas de la calidad subjetiva en TTS. Trabajo Futuro: Ampliar los conjuntos de datos y niveles de ruido para un análisis más robusto. Explorar sistemas TTS entrenados conjuntamente con algoritmos de reducción de ruido. Conclusiones Este trabajo concluye que el preprocesamiento con DeepFilterNet mejora significativamente el rendimiento de los sistemas TTS, con un aumento de 1.1 en la puntuación CMOS. Estos hallazgos destacan la importancia de la selección de algoritmos para optimizar sistemas TTS con pocos recursos. Además, adquirí valiosos conocimientos sobre evaluaciones subjetivas y el análisis estadístico necesario para extraer conclusiones significativas de los datos.\nToda la información de este estudio se encuentra en el informe académico (en inglés).\n","date":"2024-11-22T00:00:00Z","permalink":"http://localhost:1313/es/p/efecto-de-algor%C3%ADtmos-de-reducci%C3%B3n-de-rudio-en-sistemas-de-tts/","title":"Efecto de algorítmos de reducción de rudio en sistemas de TTS"},{"content":"\u0026ldquo;BassAdo\u0026rdquo; un parlante hogareño semi portable de bajo costo Este proyecto se enmarca en la materia Electroacústica II en UNTREF de la carrera Ingeniería de Sonido. La tarea era diseñar desde cero un altoparlante aplicando la teoría y conceptos explicados en clase. El trabajo se fue desarrollando durante todo el cuatrimestre con diferentes etapas a completar que se tenían que presentar con un informe. El parlante esta pensado para ser usado en un espacio grande, puede ser al aire libre, para poner música en un entorno de reunión. Se lo denomino Bassado para combinar el asado (típica comida argentina donde la gente se reune) y la palabra bass, enfatizando la característica del altoparlante.\nDiseño El objetivo es diseñar un sistema de audio hogareño con prestaciones accesibles, que permita explorar temas de interés abordados en la materia. El diseño busca lograr una predominancia en graves, característica de los sistemas comerciales, priorizando la extensión del ancho de banda en bajas frecuencias por encima de un bajo retardo de grupo y control temporal del sistema.\nCon respecto a los transductores a utilizar, el equipo contaba con unidades de la marca Yharo, que se pueden clasificar como no profesionales, consumidor-aficionado, con posibles aplicaciones en automóviles y/o sistemas hogareños. Se evalúa la respuesta en impedancia de los mismos y se selecciona un Woofer de 8” para cubrir la sección de bajas frecuencias, y dos de 4” para el rango de medias/altas.\nAl medir los parámetros de Thielle-Small de los altoparlantes se encuentra que tiene un Vas (Volumen Acústico Equivalente de Suspensión) muy alto, lo que requiere realizar un gabinete con mucho volumen para tener un buen control. Para solucionar este problema y dado la posibilidad de que se contaba con 2 Woofers de 8”, se decide hacer un altoparlante del tipo isobárico, colocando los 2 Woofers en serie acústicamente, para que tengan mayor control y poder hacer más chico el gabinete. Además, va a ser un gabinete ventilado para tener mejor respuesta en bajas.\nLa respuesta de los altoparlantes (paráemtros TS) fue obtenida en el software REW. Con esos parámetros, se simula en el software Basta! y se optimizan los parámetros para obtener la respuesta deseada. Lo principal fue sintonizar la frecuencia de resonancia del port, ya que el objetivo del altoparlante era tener una buena respuesta en bajas. El transductor tenía una fs en 45 Hz, y se busca sintonizar el port en 40 Hz controlando el largo del tubo y el volumen de aire del gabinete.\nEn base a estos resultados, se hace un modelo 3D del gabinete en el software SolidWorks con el que se manda a cortar las maderas para poder construir el gabinete.\nEl detalle de todo este proceso esta documentado en el siguiente informe de diseño.\nConstrucción Se mandaron a realizar los cortes a las maderas para hacer el gabinete según el modelo 3D y se realiza el ensamblado.\n\u0026lsaquo;\r\u0026rsaquo;\rComo se puede ver en las imágenes, se agregó lana de roca para hacer de absorbente acústico. Al realizar las mediciones nos dimos cuenta de que fue demasiado (la resonancia del port quedaba muy amortiguada) pero se le pudo sacar lana de roca hasta lograr el resultado deseado.\nMedición y Calibración Se realizaron mediciones de respuesta en frecuencia y de directividad en el laboratorio de la universidad, teníamos a disposición el siguiente equipamiento:\nPotencia Powersoft M50Q Micrófono Earthworks M50 Placa de Audio RME Fireface UCX Mesa Giratoria OUTLINE ET250-3D Con estas prestaciones y utilizando el software Arta, se pudo caracterizar la respuesta acústica de los transductores por separado (lo cual nos va a servir para simular los filtros de cruze). También se evalúa la respuesta directiva en el eje vertical y el horizontal para poder determinar cual es la mejor disposición para utilizar el dispositivo. Se realizaron gráficos de la respuesta para los dos transductores.\nTodas las respuestas y el análisis en profundidad se encuentran en el siguiente informe de medición.\nDiseño de filtro de cruce Por último, se diseña la etapa del filtro de curce. Con las mediciones realizadas previamente, se subieron los datos al software VituixCad. El objetivo del filtro de cruce es lograr una respuesta en frecuencia que suene agradable al reproducir un programa músical y que realce las bajas frecuencias. Se busca también, la mayor homogeneidad en la respuesta polar vertical.\nComo se va utilizar una etapa de potencia que requiere alimentación, se va a realizar un filtro de cruce activo con la topología Sallen-Key. Se definen una cantidad de filtros según el espacio y el costo y se realizan los ajustes en el software para obtener la respuesta deseada. Por ejemplo, para los drivers de baja frecuencia se realizó el siguiente arreglo:\nDonde:\nF1: Pasa altos fs=30 Hz | Q=0.67 F2: Pasa bajos fs=480 Hz | Q=0.5 F3: Elimina Banda 220 Hz F4: Elimina Banda 400 x Hz Antes de realizar el filtro, se probó la configuración planteada con un filtro digital para evaluar de forma práctica la respuesta del sistema según el diseño planteado. Todos los detalles de esta sección estan plasmados en el siguiente informe de filtro de cruce.\nConclusiones Con este trabajo, pudimos bajar muchos conceptos teóricos a la práctica y entender con más profundidad como es el desarrollo y los desafíos en un diseño de un sistema electroacústico.\n","date":"2024-11-20T00:00:00Z","image":"http://localhost:1313/p/building-and-design-of-a-personal-loudspeaker/front_bass_hu16513420204983711350.PNG","permalink":"http://localhost:1313/es/p/construcci%C3%B3n-y-dise%C3%B1o-de-un-altoparlante-personal/","title":"Construcción y diseño de un altoparlante personal"},{"content":"Generador Automático de Rutinas de Ejercicio Este proyecto fue el trabajo final para la asignatura Algoritmos y Programación II en la UNTREF. La idea de la aplicación es funcionar como una planificación inteligente de rutinas de ejercicio, donde el usuario ingresa ciertos criterios y el programa establece la mejor rutina para maximizar las preferencias del usuario. El programa fue escrito en el lenguaje de programación Go.\nGestión de Datos Este curso no se centra en bases de datos, por lo que optamos por usar un archivo CSV para simular una base de datos. Este archivo contiene toda la información sobre las diferentes rutinas y sus atributos. Esto sirve como un archivo de persistencia mientras que toda la lógica es manejada por el programa.\nLos diferentes ejercicios están categorizados por temáticas. Hay dos entidades en el programa que están representadas como structs en Go: los Ejercicios y las Rutinas.\nEjercicio Todos los ejercicios tienen los siguientes atributos:\nNombre: Nombre del ejercicio. Descripción: Descripción detallada del ejercicio. Duración: Duración estimada del ejercicio. Calorías: Número de calorías quemadas durante el ejercicio. Tipo: Tipo de ejercicio (por ejemplo, cardio, fuerza, flexibilidad). Grupo Muscular: Grupo muscular objetivo del ejercicio. Puntos: Puntos asignados al ejercicio según sus tipos. Dificultad: Nivel de dificultad del ejercicio. Rutinas Todas las rutinas son una colección de ejercicios. Estas se procesan como una lista enlazada de ejercicios. Además, las rutinas tienen los siguientes atributos:\nNombre: Nombre de la rutina. Ejercicios: Cadena que almacena los IDs de los ejercicios en la rutina, separados por comas. EjerciciosDisponibles: Lista enlazada de los ejercicios disponibles para crear la rutina. Algoritmo La idea es maximizar diferentes parámetros, por ejemplo, la cantidad máxima de calorías quemadas en el menor tiempo posible, o la duración mínima para un grupo muscular o tipo de ejercicio específico. Para encontrar las mejores soluciones basadas en los datos existentes, proponemos un algoritmo de programación dinámica que busca todas las posibilidades a partir de los datos y encuentra el máximo o mínimo según las especificaciones del usuario. El enfoque de programación dinámica utiliza memoria para evitar recalcular combinaciones que ya fueron computadas, lo que optimiza el algoritmo lo suficiente como para generar las rutinas en milisegundos (con el conjunto de datos de prueba evaluado).\nInterfaz de Usuario Actualmente, el programa está diseñado para ser utilizado desde la terminal como una aplicación CLI. El usuario puede crear un ejercicio y una rutina, listar las opciones disponibles y generar una rutina personalizada basada en las especificaciones que elija.\nEsta es la primera iteración del proyecto y es completamente funcional, pero planeamos crear una interfaz gráfica para que sea más amigable para el usuario. Para este propósito, sería mejor usar un framework diferente y utilizar esta aplicación en Go como backend para un servicio web o una aplicación.\nConclusiones Con este proyecto, solidifiqué mis conocimientos sobre temas de programación como estructuras de datos y algoritmos, ya que aplicamos la teoría vista en clase a un caso real. También me ayudó a familiarizarme con el lenguaje Go, que se convirtió en un lenguaje que disfruto mucho programar. El código de este proyecto está disponible en este repositorio.\n","date":"2024-04-17T00:00:00Z","permalink":"http://localhost:1313/es/p/generador-de-ruitnas-de-entrenamiento-autom%C3%A1ticas/","title":"Generador de ruitnas de entrenamiento automáticas"},{"content":"Entender un modelo moderno de deep learning no es sencillo debido a la gran cantidad de conocimiento previo necesario y al rápido ritmo de avance en este campo. En el proyecto de investigación Intercambios Transorgánicos, estamos trabajando con TTS, específicamente con el modelo FastPitch de Nvidia. He estudiado este modelo para ajustarlo (fine-tuning) al español y he compartido este proceso de investigación en una clase para ayudar a mis colegas del grupo de investigación a comprenderlo mejor.\nEntendiendo los Modelos Seq2Seq En Intercambios Transorgánicos utilizábamos previamente Tacotron2 como modelo TTS. Este modelo funciona bien, pero presenta varios problemas, principalmente durante el entrenamiento y la inferencia, debido a su naturaleza auto-regresiva. En contraste, FastPitch es un modelo no auto-regresivo (NAR). Para entender estas diferencias, exploré en profundidad los modelos seq2seq, analizando su evolución a lo largo del tiempo, y realicé un resumen rápido de los modelos de análisis de secuencias más relevantes:\nRNN LSTM Transformers Tacotron2 se basa en un modelo LSTM (AR), mientras que FastPitch utiliza Transformers (NAR). Comprender esta progresión tecnológica proporciona un contexto esencial, especialmente sobre los transformers, incluyendo la codificación posicional, un elemento clave para su naturaleza no auto-regresiva, y el mecanismo de atención.\nLa Arquitectura Transformer Comencé estudiando la arquitectura transformer, ya que es fundamental para el modelo FastPitch. Revisé recursos en línea y el influyente artículo Attention is All You Need. A continuación, algunos puntos clave que anoté durante mi estudio:\nMecanismo de Auto-Atención\nPropósito: Focalizar dinámicamente en diferentes partes de la secuencia de entrada. Mecanismo: Query (Q), Key (K), Value (V): Derivados de las embeddings de entrada. Los puntajes de atención se calculan como el producto punto de Q y K, escalado por la raíz cuadrada de la dimensión. Los puntajes se normalizan con softmax para generar pesos de atención. Se calcula una suma ponderada de V basada en estos pesos para producir la salida. Atención Multi-Cabezal\nPropósito: Capturar diferentes relaciones entre tokens aplicando múltiples mecanismos de auto-atención en paralelo. Mecanismo: Las salidas de las múltiples cabezas de atención se concatenan y se transforman linealmente. Codificación Posicional\nPropósito: Añadir información sobre el orden de los tokens en la secuencia, compensando la falta de un concepto incorporado de orden secuencial en los Transformers (a diferencia de los RNNs). Mecanismo: Se añade un vector fijo o aprendible a las embeddings de entrada. Estructura Codificador-Descodificador\nCodificador: Procesa la secuencia de entrada en representaciones ricas en contexto. Componentes: Auto-atención multi-cabezal Red neuronal feed-forward (FFN) Normalización por capas y conexiones residuales Descodificador: Genera la secuencia de salida atendiendo tanto a las salidas del codificador como a los tokens generados previamente. Componentes: Auto-atención multi-cabezal enmascarada (evita atender a tokens futuros) Atención multi-cabezal sobre las salidas del codificador FFN, normalización por capas y conexiones residuales Red Feed-Forward (FFN)\nPropósito: Introducir no linealidad y procesar cada token de manera independiente. Mecanismo: Dos capas lineales con una activación ReLU entre ellas. Normalización por Capas y Conexiones Residuales\nPropósito: Estabilizar el entrenamiento y mejorar el flujo de gradientes al normalizar las entradas de cada capa y añadir conexiones de salto. FastPitch Con la teoría cubierta, examiné cada sección de la arquitectura de FastPitch en detalle. Ofrecí una breve explicación sobre las embeddings de palabras y la codificación posicional, ya que son temas complejos, y quise mantener la clase concisa.\nFastPitch convierte texto en espectrogramas mel, que luego son transformados en audio por otro modelo (en nuestro caso, HiFi-GAN). La secuencia de entrenamiento incluye los siguientes pasos:\nTexto a embeddings de palabras Concatenación de embeddings de palabras con espectrogramas mel Codificación posicional y FFT (bloque Transformer Feed-Forward) Predicción de tono Predicción de la duración de los fonemas Otro bloque FFT Capa completamente conectada Salida del espectrograma mel En cada bloque, presenté las ecuaciones correspondientes y proporcioné explicaciones cualitativas sobre su rol en el modelo. Por ejemplo, la predicción de la duración de los fonemas es crucial para alinear la duración de un fonema con la duración esperada en el espectrograma.\nClase Online Finalmente, resumí los puntos más importantes y realicé una clase online para compartir estos conceptos con mis colegas. Puedes verla aquí:\n","date":"2023-08-23T00:00:00Z","image":"http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/front_hu1732994514150025714.png","permalink":"http://localhost:1313/es/p/entendiendo-fastpitch-y-la-arquitectura-transformers/","title":"Entendiendo FastPitch y la arquitectura Transformers"},{"content":"En el curso de Procesamiento Digital de Señales (DSP) en la UNTREF, exploramos el plano Z para el diseño de filtros con variables discretas. Durante la clase, el profesor presentó una herramienta construida en MatLab que visualiza los gráficos de transferencia y fase en relación con las posiciones de los ceros y polos en el plano Z.\nInspirado en esto, decidí recrear esta herramienta en Python. Aprovechando mi experiencia con la biblioteca PyGame, logré construir una aplicación en tiempo real que permite mover los polos y ceros, permitiendo a los usuarios ver cómo cambia la función de transferencia en tiempo real.\nCómo Funciona Primero, mapeo las posiciones de los píxeles en el plano Z a coordenadas de acuerdo con la representación del círculo unitario. Luego, construyo la función de transferencia, donde cada cero $z_{n}$ es un término en el numerador y cada polo $z_{i}$ es un término en el denominador. Con la función de transferencia $H(z)$, puedo graficar tanto el magnitud como el gráfico de fase, que también son representaciones mapeadas de la respuesta normalizada dentro de la aplicación.\n$$ H(z) = \\frac{\\sum_{n=0}^{N} (z - z_{n})}{\\sum_{i=0}^{N} (z - z_{i})}\r$$Cada fotograma recalcula la función de transferencia. El programa funciona de manera eficiente porque almaceno los ceros y polos en arreglos, lo que permite cálculos más rápidos gracias a numpy, que ya está altamente optimizado. Esto permite visualizar los cambios en tiempo real y proporciona una comprensión más intuitiva del comportamiento del plano Z. El siguiente fragmento de código muestra como se calcula la magnitud y fase utilizando exponenciales complejas.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def e(w, root): return (np.exp(1j*w) - root) def transfer_function(zeros, poles): w = np.linspace(-np.pi, np.pi, RES) for zero in zeros: num = e(w, zero) for pole in poles: den = e(w, pole) H_z = num/den mag = np.abs(H_z) ang = np.angle(H_z) return mag, ang Este proyecto está diseñado como material educativo, proporcionando a los estudiantes una herramienta práctica para comprender mejor las interacciones entre los polos y ceros en el plano Z. No está destinado como una herramienta profesional para el diseño de filtros.\nFuncionalidad La aplicación ofrece las siguientes características:\nMostrar y mover polos/ceros: Los usuarios pueden seleccionar y mover los polos y ceros en el plano Z. Cuando la opción de simetría está activa, todos los polos y ceros seleccionados o movidos se colocan de manera simétrica respecto al eje imaginario.\nOrden: Los usuarios pueden ajustar el orden de los polos y ceros desplazando la rueda del mouse. El orden puede incrementarse o disminuirse, con soporte hasta un orden de 4. El color de cada polo o cero cambia según su orden.\nInformación: Al pasar el cursor sobre un polo o cero, se muestra información sobre su posición, simetría y orden.\nZoom: El usuario puede acercar o alejar el plano Z usando los botones de más y menos.\nEliminar: El ícono de la papelera elimina todos los polos y ceros del plano. Los usuarios también pueden eliminar polos o ceros individuales haciendo clic derecho sobre ellos.\nGráfico de Magnitud: El espectro de magnitud mostrado en la aplicación está normalizado. Esta decisión asegura que el enfoque permanezca en la forma de la magnitud, haciendo que el gráfico sea visualmente significativo para los usuarios. Sin embargo, no captura la diferencia en los valores máximos.\nGráfico de Fase: La fase se muestra sin envolver entre $-\\pi$ y $\\pi$.\nDemo Aquí tienes una demostración rápida de la aplicación en acción:\nEl código fuente de este proyecto se encuentra en este repositorio.\n","date":"2023-08-12T00:00:00Z","image":"http://localhost:1313/p/z-plane-visualizer/frontz_hu11319941584083790054.PNG","permalink":"http://localhost:1313/es/p/z-plane-visualizer/","title":"Z-Plane Visualizer"},{"content":"Los algoritmos de modificación de escala temporal se utilizan para acelerar o desacelerar la velocidad de reproducción de un audio. Cuando se cambia la tasa de muestreo de un audio, la velocidad cambia, pero también lo hace el tono (al acelerar el audio, suena con un tono más alto). Existen diferentes algoritmos que permiten modificar la velocidad del audio manteniendo constante el tono.\nLa principal referencia para este estudio es el siguiente artículo, donde se describen en detalle los distintos algoritmos.\nA Review of Time-Scale Modification of Music Signals.\n— Jonathan Driedger y Meinard Müller1\nComparación de algoritmos Existen dos algoritmos principales, el Overlap-and-add (OLA) y el Phase Vocoder (PV). Ambos logran buenos resultados bajo diferentes señales y condiciones. Para aprovechar las ventajas de ambos métodos, se utiliza una implementación final basada en la Separación Armónico-Percusiva (HPS), que combina ambos algoritmos y logra los mejores resultados.\nOLA Este método trabaja en el dominio del tiempo, superponiendo secciones del audio (ventanas) y reorganizándolas para lograr un cambio deseado en la velocidad. Este método funciona bien para señales percusivas, pero introduce artefactos cuando se utiliza con señales armónicas o tonales.\nPV Este método opera en el dominio de la frecuencia, combinando fragmentos de audio en este dominio para lograr el cambio deseado en el tiempo. Utiliza el principio del vocoder de fase para propagar la fase entre ventanas, garantizando la continuidad al aplicarse a señales armónicas. Sin embargo, no es efectivo para señales percusivas, ya que el proceso de propagación de fase elimina los transitorios en las señales.\nCreé visualizaciones usando Manim para mejorar mi presentación en clase. El primer video muestra cómo el algoritmo PV alinea las ventanas para garantizar transiciones suaves en la señal generada a lo largo del tiempo. Para lograrlo, se aplica una ventana gaussiana que mantiene la continuidad y suavidad, incluso al inicio y al final de la secuencia.\nEl segundo video muestra los efectos de aplicar el algoritmo PV a una señal que contiene transitorios.\nComo predice la teoría, los transitorios desaparecen porque el algoritmo PV interrumpe la alineación vertical de la fase. Aunque estos ejemplos utilizan señales idealizadas, demuestran de manera efectiva las principales fortalezas y limitaciones del algoritmo.\nHPS Para utilizar ambos métodos con las señales ideales, se emplea el algoritmo HPS. Este algoritmo separa la señal en sus componentes armónicas y percusivas. Funciona comparando la continuidad de la señal en la representación STFT, utilizando un filtro que compara la presencia vertical contra la horizontal en el espectrograma. Con un umbral, se puede definir una máscara binaria sobre el espectrograma para separar las partes percusivas de las secciones armónicas.\nResultados Implementamos con éxito todos los algoritmos y los comparamos, verificando los contenidos teóricos presentados en el artículo de referencia. Durante el proceso, desarrollamos un conjunto de herramientas para utilizar estos algoritmos con el lenguaje de programación Python. Todo el código está disponible en este repositorio.\nPresentación académica El estudio fue presentado junto a mis compañeros en las JAAS (Jornadas de Acústica, Audio y Sonido). Las principales ideas y conclusiones se expusieron en la conferencia. En el siguiente informe se encuentran todos los detalles y análisis realizados para este proyecto (en español).\nALGORITMOS DE MODIFICACIÓN DE ESCALA TEMPORAL.\n— Matías Di Bernardo; Matías Vereertbruhggen; Sebastían Carro2\nA Review of Time-Scale Modification of Music Signal paper.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJAAS 2023 - Algoritmos de Modificación de Escala Temporal paper.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-03-09T00:00:00Z","image":"http://localhost:1313/p/time-scale-modification-algorithms/tsm_hu16003083865389877412.PNG","permalink":"http://localhost:1313/es/p/algor%C3%ADtmos-de-modificaci%C3%B3n-de-escala-temporal/","title":"Algorítmos de modificación de escala temporal"},{"content":"Usé este proyecto como introducción al Aprendizaje por Refuerzo (Reinforcement Learning). Habiendo trabajado principalmente con aprendizaje supervisado y no supervisado, quería comenzar con un proyecto pequeño y sencillo para comprender rápidamente las ideas principales y crear algo divertido. Seguí este video de YouTube como referencia, el cual explica cómo usar Aprendizaje por Refuerzo (RL) para entrenar un modelo capaz de jugar al juego de la serpiente. Para hacerlo más desafiante, apliqué la misma red al juego 2048.\nCódigo El código tiene tres componentes principales:\nEl Juego: Implementa la lógica del juego y la interfaz gráfica (GUI). Agente: Controla la jugabilidad. Modelo de IA: Una red neuronal que aprende a jugar y guía al agente. Estado del Juego Modelé el estado del juego como una matriz de 4x4 que representa el tablero. Las acciones del juego están representadas por un vector con las siguientes posibilidades:\nIzquierda: [1, 0, 0, 0] Derecha: [0, 1, 0, 0] Arriba: [0, 0, 1, 0] Abajo: [0, 0, 0, 1] Recompensa El modelo de RL funciona con recompensas para cuantificar cuándo el agente actúa bien o mal. Inicialmente, definí las siguientes recompensas:\n+10 puntos: Cuando el agente duplica los puntos. Esto es crucial porque, en 2048, los puntos aumentan exponencialmente. -10 puntos: Cuando se pierde el juego. Esto sirve como una penalización sencilla. Al principio, el modelo mostraba una mejora lenta. Para solucionarlo, añadí una recompensa adicional:\n+2 puntos: Cuando se incrementan los puntos. Esta recompensa incentiva acciones que maximicen la limpieza del tablero y el progreso. Cómo Funciona el Modelo El modelo es una red neuronal lineal simple. Toma el estado del juego como entrada y predice la mejor acción siguiente para maximizar la recompensa.\nLas recompensas se gestionan utilizando una técnica llamada Q-Learning. Un valor Q representa la calidad de una decisión basada en la función de pérdida. La función de pérdida se deriva de la Ecuación de Bellman:\n$$\rNewQ(s, a) = Q(s, a) + \\alpha [R(s, a) + \\lambda \\, \\text{max}Q'(s', a') - Q(s, a)]\r$$Donde:\n$Q(s, a)$: El valor Q para un estado y acción específicos. $\\alpha$: Tasa de aprendizaje. $R(s, a)$: Recompensa para un estado y acción específicos. $\\lambda$: Tasa de descuento. $\\text{max}Q\u0026rsquo;(s\u0026rsquo;, a\u0026rsquo;)$: Recompensa futura máxima esperada. Experimentos y Resultados Prueba Aleatoria como Línea Base Para establecer una línea base, probé el puntaje promedio que se puede lograr tomando acciones aleatorias. Después de 1,000 iteraciones, el puntaje promedio fue de 170, muy por debajo de los 2048 puntos necesarios para ganar el juego.\nResultados Iniciales Mis primeros intentos fueron desalentadores. El modelo tenía un desempeño peor que los movimientos aleatorios. Aquí algunos resultados iniciales:\nJuego 1,000 | Puntaje: 208 | Récord: 416 | Puntaje Promedio: 200 | Recompensa: 640 Juego 1,000 | Puntaje: 116 | Récord: 346 | Puntaje Promedio: 161 | Recompensa: 310 Juego 1,000 | Puntaje: 112 | Récord: 348 | Puntaje Promedio: 146 | Recompensa: 320 En estos intentos, el agente desarrolló una estrategia subóptima de llenar el tablero antes de incrementar los puntos.\nMejoras Tras experimentar con los parámetros de recompensa, me centré en la fase de exploración. Inicialmente, el parámetro de juegos de exploración, que elige movimientos aleatorios, estaba configurado en 25 juegos. Al aumentar este parámetro, el agente pudo explorar más estrategias, logrando mejores resultados:\nJuego 1,000 | Puntaje: 478 | Récord: 478 | Puntaje Promedio: 230 Juego 1,000 | Puntaje: 514 | Récord: 964 | Puntaje Promedio: 382 A medida que el modelo mejoraba, extendí el entrenamiento a más juegos:\nJuego 3,796 | Puntaje: 770 | Récord: 1,366 | Puntaje Promedio: 469 Juego 4,852 | Puntaje: 631 | Récord: 1,462 | Puntaje Promedio: 483 Finalmente, con 200 juegos de exploración y 5,000 juegos de entrenamiento, los resultados fueron los siguientes:\nJuego 5,000 | Puntaje: 840 | Récord: 1,678 | Puntaje Promedio: 512 Aunque el modelo no logró ganar el juego, quedé satisfecho con el progreso. Creo que con más tiempo de entrenamiento (el último intento duró 4 horas) y capas adicionales en la red, sería posible ganar el juego con esta arquitectura.\nDemo Aquí tienes una demostración de la aplicación, mostrando el proceso de aprendizaje del agente:\nImplementé atajos de teclado para controlar la velocidad del juego, ofreciendo tres opciones:\nRápida: Para un entrenamiento acelerado. Lenta: Para observar y analizar el progreso del agente y sus errores. Media: Casi no la utilicé. El código completo para este proyecto está disponible en este repositorio.\n","date":"2023-02-17T00:00:00Z","image":"http://localhost:1313/p/ai-learns-to-play-2048-game/game_img_hu1650208459137967495.jpg","permalink":"http://localhost:1313/es/p/ia-aprende-a-jugar-2048/","title":"IA aprende a jugar 2048"},{"content":"Esta investigación se desarrolla en el marco de la materia Metodología de la Investigación en UNTREF. En el artículo se pretende comparar las diferencias entre tres tipos de transformaciones tiempo-frecuencia:\nTransformada de Fourier (FT) Transformada Wavelet (WT) Transformada Huang-Hilbert (HHT) El objetivo del trabajo es entender las diferencias entre estos tipos de transformaciones y profundizar mi conocimiento en el procesamiento de señales.\nObjetivo El objetivo general de la investigación es determinar con qué herramienta de análisis espectral se logra una mayor precisión en la tarea de detección de tono.\nPara alcanzar este objetivo, se plantean los siguientes puntos a completar:\nDeterminar los parámetros necesarios para representar la señal en el dominio espectral según cada caso. Elegir un algoritmo que identifique el tono de la señal en base a su representación espectral. Generar los datos (señales de audio) con los cuales se realizará la comparación. Evaluar los datos generados con los distintos métodos de análisis y aplicar procesos estadísticos para validar los resultados. Establecer una medida de precisión en la tarea de detección de tono. Comparar los resultados de los distintos análisis y determinar cuál es el que consigue una mayor precisión en la detección de tono. Se eligió la tarea de detección de tono porque es una de las aplicaciones principales de este tipo de transformadas.\nAlgoritmos El análisis teórico de todas las transformadas se realiza en el dominio continuo, pero para llevar a cabo los experimentos y comparaciones se trabaja en el dominio discreto, de forma que todos los cálculos se realizan digitalmente.\nFFT La FFT es un algoritmo que optimiza la DFT (Transformada de Fourier de tiempo discreto). Con este algoritmo se obtiene la representación espectral de la señal de acuerdo al análisis de Fourier, descomponiendo una señal compleja en una suma de senos o cosenos. La DFT se calcula mediante la fórmula:\n$$\rX_k = \\sum_{n=0}^{N-1} e^{-i\\frac{2\\pi}{N}kn} x_n\r$$Donde \\( N \\) es la cantidad de muestras de la señal y \\( k \\) son los números naturales de \\( 0 \\) hasta \\( N – 1 \\).\nWT La Transformada Wavelet (WT) utiliza una función ondulatoria (wavelet) y aplica una convolución entre la señal y la función de onda elegida para determinar si esa forma ondulatoria está presente en la señal. La onda se deforma en frecuencia y amplitud, permitiendo que una sola función ondulatoria recree todo el espectro de interés.\nPara esta investigación se empleará la CDWT (Cyclic Discrete Wavelet Transform), la implementación más común al discretizar la WT. Conceptualmente, esta transformada extiende el análisis de Fourier utilizando una base de funciones ondulatorias en lugar de senos y cosenos. Se calcula así:\n$$\rWf[n, a^j] = \\sum_{m=0}^{N-1} f[m] \\psi_j[m-n]\r$$Donde \\( N \\) es la cantidad de muestras de la señal, \\( \\psi \\) es la función ondulatoria y \\( j \\) representa la deformación de la onda según el banco de ondas seleccionado.\nHHT Por último, se empleará la Transformada Huang-Hilbert (HHT), que utiliza la Descomposición Empírica Modal (EMD) para descomponer la señal en subseñales relevantes. En lugar de descomponer en funciones senoidales u ondulatorias, el EMD encuentra funciones modales intrínsecas (IMF) específicas de cada señal.\nLa relación entre las IMF y la frecuencia original se establece con la ecuación:\n$$\rz(t) = f(t) + i H\\{ f(t) \\}\r$$Donde \\( f(t) \\) es una IMF y \\( H \\) es la Transformada Hilbert. Esto permite proyectar la señal al eje imaginario y extraer la amplitud y la fase de cada instante, construyendo la representación espectral. Como una señal generalmente tiene múltiples IMF, este proceso se repite para todas y se suman para obtener el espectro completo.\nProcedimiento Se analizará la relación entre los tipos de representación espectral y la precisión en la detección de tono.\nPrimero, se seleccionarán los parámetros para las distintas transformadas, como el número de muestras para el ventaneo temporal, que determina la resolución temporal y frecuencial.\nPara comparar los métodos, se generarán datos representativos de distintos casos de interés, modelando cuatro tipos de señales:\nMonótonas: Una sola nota correspondiente a la \\( F_0 \\). Politonales: Múltiples notas, donde la armonía determina la \\( F_0 \\). Transiciones lentas: Cambios de \\( F_0 \\) graduales. Transiciones rápidas: Cambios de \\( F_0 \\) abruptos. Se comparará el valor real \\( V(t) \\) con el resultado \\( P(t) \\) de cada transformada, integrando la diferencia temporalmente para calcular la precisión.\nResultados En esta etapa, solo se requería completar el plan de investigación detallando el procedimiento y los métodos de análisis. Para ello, se generaron datos de ejemplo (dummy data) y se validaron estadísticamente los resultados esperados.\nEl gráfico muestra la precisión obtenida con las tres transformadas en función del tipo de señal. Se espera que la Transformada Wavelet (WT) supere a la Transformada de Fourier (FT), y que la Transformada Huang-Hilbert (HHT) sea más precisa en general.\nConclusiones En la tarea de detección de tono por análisis frecuencial, la Transformada Huang-Hilbert (HHT) ofrece, en la mayoría de los casos, mayor precisión que la Transformada Rápida de Fourier (FFT) y la Transformada Wavelet Cíclica (CDWT).\nLa magnitud de esta diferencia depende del tipo de señal analizada, siendo las señales con transiciones rápidas las menos beneficiadas por el cambio de transformada, mientras que las señales politonales muestran las mejoras más significativas al emplear la HHT.\nEn este proyecto, profundicé mis conocimientos en procesamiento de señales y comprendí los fundamentos para usar herramientas como la WT y la HHT según las características de la señal.\nMás detalles del trabajo están disponibles en el siguiente informe.\n","date":"2022-11-11T00:00:00Z","image":"http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/fourier_hu9759776206134608272.jpg","permalink":"http://localhost:1313/es/p/comparaci%C3%B3n-de-diferentes-transformaciones-tiempo-frecuencia/","title":"Comparación de diferentes transformaciones tiempo-frecuencia"},{"content":"Este proyecto comenzó como el trabajo final para un seminario de la UNTREF llamado Seminario en Aplicaciones de Redes Neuronales en la recuperación de información musical. El objetivo era utilizar una Red Neuronal Siamés (SNN, por sus siglas en inglés) en un contexto diferente al visto en clase (detección de similitud musical).\nPara el proyecto final de este curso, desarrollamos un modelo SNN desde cero utilizando el framework Keras y la arquitectura SincNet para reducir la dimensionalidad del audio, logrando buenos resultados. Más adelante, para expandir este proyecto, probé otro enfoque utilizando Wav2Vec para la reducción de dimensionalidad y reimplementé todo el proyecto en el framework PyTorch. Sin embargo, este intento arrojó resultados subóptimos, lo que indica que la reducción de dimensionalidad con Wav2Vec perdió información crítica necesaria para la tarea de diarización de locutores.\nTarea de Diarización de Locutores El objetivo de un modelo de diarización de locutores es identificar diferentes hablantes en un flujo de audio que contiene múltiples voces. Por ejemplo, en un podcast con dos personas (A y B), el modelo debe determinar los pasos de tiempo en los que el hablante A está hablando y los pasos de tiempo en los que el hablante B está hablando (e identificar implícitamente los períodos de silencio). Estos modelos son extremadamente útiles para la edición de audio y el análisis de largas secuencias de audio con múltiples hablantes.\n¿Por qué Redes Neuronales Siamés? En clase, exploramos la arquitectura siamés para comparar similitudes entre piezas musicales, desarrollando una herramienta capaz de identificar versiones de canciones famosas.\nUna Red Neuronal Siamés consiste en dos o más subredes idénticas que comparten los mismos pesos y parámetros. Está diseñada para comparar pares de entradas y medir su similitud, generalmente utilizando una métrica de distancia como la distancia euclidiana. Cada subred procesa una entrada, y las salidas se combinan para calcular un puntaje de similitud.\nCon esta comparación de similitudes en mente, quisimos aplicar estas redes a la tarea de diarización de locutores. La idea era generar embeddings de hablantes a partir de audio utilizando un modelo preentrenado y comparar las salidas de diferentes segmentos de audio. Basándonos en el puntaje de similitud, buscamos identificar los segmentos donde diferentes hablantes están hablando.\nExperimentos Probé dos métodos diferentes para la extracción de características de audio que servirían como embeddings de hablantes.\nImplementación en Keras con SincNet En este enfoque, utilizamos la arquitectura SincNet para extraer características específicas de los hablantes a partir del audio. SincNet aplica funciones sinc aprendibles como sus filtros, que son particularmente adecuadas para el procesamiento de audio, ya que imitan los filtros de paso de banda tradicionales. Estas características se introdujeron en la Red Neuronal Siamés, que comparó pares de segmentos de audio para calcular sus puntajes de similitud. El modelo se entrenó con conjuntos de datos de audio etiquetados y observamos un rendimiento sólido al agrupar segmentos de audio por hablante, logrando límites claros entre diferentes hablantes.\nUn informe con los resultados puede encontrarse en el siguiente notebook de Jupyter (en español).\nImplementación en PyTorch con Wav2Vec Para este método, utilicé Wav2Vec, un modelo preentrenado poderoso para la extracción de embeddings profundos de audio. A diferencia de SincNet, los embeddings de Wav2Vec se derivan del aprendizaje autosupervisado, capturando representaciones de alto nivel del audio. Estos embeddings se usaron en la Red Neuronal Siamés para comparaciones de similitud. Sin embargo, los resultados fueron subóptimos para la tarea de diarización. Parece que Wav2Vec, aunque excelente para tareas de reconocimiento de voz, perdió algunos detalles específicos del hablante necesarios para distinguir entre diferentes voces en nuestro contexto.\nResultados Los experimentos demostraron que la elección del método de extracción de características es crucial para la diarización de locutores. La implementación en Keras con SincNet superó a la implementación en PyTorch con Wav2Vec, mostrando mayor precisión al identificar transiciones entre hablantes. Esto sugiere que la extracción de características específicas para la tarea, como SincNet, es más efectiva que los embeddings de propósito general como Wav2Vec para la diarización de locutores.\nEl código de este proyecto está disponible en este repositorio.\nConclusiones Este proyecto fue una de mis primeras experiencias con modelos de aprendizaje profundo, donde apliqué mis conocimientos a un problema sin seguir un artículo específico o utilizar un modelo preentrenado. Exploré diferentes soluciones y concluí sobre la importancia de la extracción de características y la selección del modelo.\nTambién me ayudó a familiarizarme con la sintaxis de los frameworks de aprendizaje profundo más populares y a solidificar mi comprensión en el proceso.\n","date":"2022-11-04T00:00:00Z","permalink":"http://localhost:1313/es/p/evaluaci%C3%B3n-de-diferentes-modelos-de-separaci%C3%B3n-del-hablante/","title":"Evaluación de diferentes modelos de Separación del Hablante"},{"content":"Este proyecto es el trabajo final de la clase Acústica y Psicoacústica II, donde se nos encargó rediseñar un auditorio existente. El objetivo era aplicar la teoría vista en clase para crear un auditorio optimizado desde el punto de vista acústico. Para nuestro proyecto, elegimos rediseñar el Royal Albert Hall en Londres. Esto representó un desafío particular debido a las grandes dimensiones del auditorio, lo que dificulta asegurar que el sonido llegue de manera uniforme a todos los espectadores.\nIdeas Principales del Rediseño El rediseño buscó preservar el concepto original del auditorio, incluyendo su gran volumen y capacidad de asientos, mientras se introducían cambios críticos para mejorar su acústica. Aunque el enfoque principal fue la mejora acústica, el rediseño también consideró otros factores esenciales, como las líneas de visión y una distribución adecuada de los asientos.\nA pesar de la intención de mantener las dimensiones originales del auditorio, su volumen resultó ser demasiado grande para lograr un tiempo de reverberación óptimo. Para abordar este problema, el rediseño incorporó un techo intermedio para reducir el volumen del techo esférico, y se redujo el área principal de asientos. Estos cambios permitieron obtener un mejor tiempo de reverberación en la sala, como se ilustra en la sección transversal a continuación.\nDetalles de Construcción y Regulaciones Para garantizar un rediseño factible y funcional, se consideraron cuidadosamente los siguientes aspectos clave:\nDistribución de asientos Espaciado de los pasillos Optimización de las líneas de visión Comodidad del escenario Tratamiento Acústico El tratamiento acústico fue la parte más crítica de este estudio y se centró en dos aspectos principales: reflexiones y tiempo de reverberación.\nReflexiones El análisis de las reflexiones es esencial para la experiencia acústica del público. El Royal Albert Hall original cuenta con un techo esférico que centraliza las reflexiones, creando efectos acústicos indeseables. Para mitigar esto, el rediseño incorporó un techo intermedio con una geometría específica diseñada para distribuir las reflexiones de manera uniforme entre el público.\nEl diseño escalonado del techo garantiza reflexiones adecuadas para todas las filas de asientos. En el balcón principal, se abordaron dos reflexiones específicas para compensar el menor nivel de presión sonora (SPL) debido a la gran distancia desde el escenario, como se muestra en la imagen a continuación.\nTambién se optimizaron las reflexiones laterales mediante ajustes en la geometría del escenario y las paredes de los balcones laterales.\nAdemás, el rediseño buscó minimizar el ITDG (Initial Time Delay Gap) en los diferentes puntos del público.\nMateriales y Tiempo de Reverberación El rediseño siguió las recomendaciones del libro Acoustic Absorbers and Diffusers para lograr un equilibrio entre absorción, difusión y reflexiones especulares. Se utilizaron materiales reflectantes en el techo y en partes de los balcones laterales para garantizar reflexiones especulares efectivas. Para reducir el tiempo de reverberación (RT), se aplicaron materiales con coeficientes de absorción más altos en otras superficies.\nUsando los materiales seleccionados y la ecuación de Sabine, calculamos el RT estimado del auditorio. El tiempo de reverberación para las diferentes frecuencias se muestra a continuación:\nEl RT calculado para frecuencias medias es de 2,51 segundos. Aunque este valor está ligeramente por encima del máximo recomendado de 2,4 segundos para una acústica óptima, es aceptable dado el gran volumen del auditorio.\nModelado 3D Renderizamos el auditorio rediseñado utilizando el software SketchUp. A continuación, se presentan algunas visualizaciones:\n\u0026lsaquo;\r\u0026rsaquo;\rConclusiones Rediseñar el Royal Albert Hall para mejorar su acústica mientras se preserva su esencia original presentó desafíos significativos. El proyecto requirió soluciones innovadoras para abordar los problemas acústicos sin comprometer el diseño icónico del auditorio. Aunque fueron necesarios algunos cambios, el resultado final demuestra un rediseño cuidadosamente pensado que mejora la acústica mientras mantiene el carácter histórico del Royal Albert Hall. Este proyecto también profundizó nuestra comprensión de los conceptos de acústica y diseño de auditorios.\nUna descripción detallada de este proyecto está disponible en el siguiente artículo (en inglés).\n","date":"2022-06-22T00:00:00Z","image":"http://localhost:1313/p/theather-acoustic-design/front_hu7206688613610463615.PNG","permalink":"http://localhost:1313/es/p/dise%C3%B1o-de-un-auditorio/","title":"Diseño de un auditorio"}]