[{"content":"At Intercambios Transorgánicos, we continuously work with state-of-the-art Deep Learning models. To achieve this, we stay up to date with the latest developments in the field. In this presentation, the objective was to explain the fundamentals of the F5-TTS model, a cutting-edge speech synthesis system. During the session, the concept of generative artificial intelligence and flow-based models was introduced, as F5-TTS is based on this technique. The purpose is to share this knowledge with the team and with anyone interested in getting started with Text-to-Speech (TTS) models.\nIntroduction - Generative AI and TTS The class was structured as follows:\nIntroduction to TTS models: Description of the different models used, their main characteristics, and the reasons behind changing approaches. Evolution of TTS models: A graph-based survey illustrating the progression of various TTS technologies. Paradigm shift in language models: Explanation of how using large datasets has enabled better generalization in language models. Example of generalization: Illustrative cases, such as generating an image of an astronaut riding a horse, audio examples, and synthesis of specific accents (e.g., an angry Cordoban speaker). Change in dataset structuring: Traditional approach: Use of a single high-quality speaker with a large amount of data (model trained for each speaker). Current and future approach: Use of diverse datasets with labels that enable training models capable of understanding speech concepts and generalizing across multiple tasks (VoiceBox examples). Computational cost: Discussion of the increasing model size and training difficulty, similar to large-scale language models (LLMs). Explanation of Generative AI: Introduction to generative models and the concept of underlying distribution. Explanation of Transformers and their function in classification with context. Clarification that while Transformers are Non-Autoregressive (NAR) during training, they operate Autoregressively (AR) in inference. Difference between classification models and generative models based on dimensionality reduction and latent variables. TTS Models: VoiceBox, E2, and F5 This section details the key advancements in TTS models leading to the development of F5-TTS.\n1. VoiceBox TTS - In Context Learning The initial architecture of VoiceBox is introduced, focusing on generalization through In Context Learning. This approach allows the model to infer information based on prior examples. To achieve this, it is given an audio sample with a missing segment, forcing it to complete it coherently.\n\u0026ldquo;We show that Voicebox’s text-guided speech infilling approach is much more scalable in terms of data while subsuming many common speech generative tasks.\u0026rdquo;\nThis method, introduced by Meta in VoiceBox, represents a paradigm shift by demonstrating that TTS models can benefit from techniques previously used in language and image models.\nHowever, the model still relies on classical TTS techniques such as phoneme duration prediction and Forced Alignment to map phonemes to Mel spectrograms, as their lengths do not match those of text tokens.\n2. E2 TTS - Filler Tokens E2 TTS simplifies many of the architectural decisions of VoiceBox, particularly in handling the differing lengths of Mel spectrograms and text. Instead of using forced alignment, it introduces Filler Tokens, which allow text and audio dimensions to be equalized more efficiently. This approach makes it easier for the model to learn temporal associations between the two elements.\nAnother significant change is that the model directly processes text instead of phonemes. While this might seem disadvantageous (since homographic words can have different pronunciations), context provides enough information to infer the correct pronunciation, similar to how humans process natural language.\n3. F5 TTS - Open Source Although E2 TTS is a major step forward, its training is slow due to the need to infer associations that were previously guided. F5 TTS optimizes the previous model with the following improvements:\nUse of ConvNeXt: Enhances text processing. Architectural change: Replaces the U-Net network with DiT (Diffusion Transformer). Sway Sampling: Optimizes the inference process. Additionally, F5-TTS is open-source, facilitating adoption and improvement by the community.\nFlow Matching - Theoretical Foundations To understand how F5-TTS works, it is essential to review the models it is based on, known as Flow Matching Models. These models have evolved from previous techniques in probabilistic distribution modeling.\n1. Normalizing Flow Normalizing Flows are a family of probabilistic models that transform a simple distribution into a more complex one through a series of invertible and differentiable functions. They are used to model high-dimensional data distributions and generate realistic samples with precise control over probability.\n2. Residual Flow Residual Flows extend Normalizing Flows by allowing more flexible transformations without strictly adhering to the invertibility condition. This is achieved through residual networks that approximate transformation functions without structural constraints.\n3. Continuous Normalizing Flows (CNF) Continuous Normalizing Flows reformulate Normalizing Flows using ordinary differential equations (ODEs). Instead of applying discrete transformations in successive steps, these models learn a continuous dynamic in the latent space, allowing for greater expressiveness and computational efficiency.\n4. Flow Matching Flow Matching is a technique that optimizes the transformation of a reference distribution into a target distribution by minimizing a specific cost function. It differs from previous approaches by not requiring explicit density function calculations, making it particularly useful for large-scale generative models such as F5-TTS.\nOnline Presentation All of this sections are explained in detail on the online class avaliable here (in Spanish):\n","date":"2025-03-24T00:00:00Z","image":"http://localhost:1313/p/generative-ai-flow-matching-and-tts/front2_hu7705949593533117439.PNG","permalink":"http://localhost:1313/p/generative-ai-flow-matching-and-tts/","title":"Generative AI, Flow Matching and TTS"},{"content":"THD+N Meter A THD+N meter measures the harmonic distortion of a device. In audio, this is crucial for assessing the quality of equipment. This project was developed as part of the subject \u0026ldquo;Instrumentos y Mediciones Electrónicas\u0026rdquo; at UNTREF. In this course, students design and develop various electronic instruments, and projects are carried forward by successive student groups. This particular project was already underway, and our focus was on developing a phase shifter to align two signals.\nTeory of THD measurement To measure the distortion of a system, a sinusoidal signal with minimal distortion is input to the device under test. The goal is to evaluate how much distortion the device adds to the signal. This is quantified by measuring the power of the original signal and the power of the signal after passing through the device, excluding the fundamental harmonic.\nA differential amplifier is used to subtract the device\u0026rsquo;s output signal from the reference signal, leaving only the higher-order harmonics.\nThe THD+N is then calculated as a percentage using the following equation:\n$$\rTHD+N = \\frac{V_{filt}}{V_{tot}} \\cdot 100\r$$Where: $V_{filt}$ is the RMS value of the filtered signal (excluding the fundamental harmonic). $V_{tot}$ is the RMS value of the original signal.\nPhase Shifter To achieve effective cancellation in the differential section, precise phase and gain adjustments of the two signals are required. The gain adjustment had been successfully implemented by the previous group working on the project. However, achieving the necessary 360-degree phase rotation across the entire audible frequency range (20 Hz to 20 kHz) presented a challenge.\nTo address this, we designed two all-pass filters in series and calculated the component values to achieve the desired phase rotation over the target frequency range.\nThe design was modular to facilitate seamless integration with the previous stages.\nWe tested the circuit on a protoboard before designing the PCB using Altium Designer.\nThe device The device features several controls for adjusting phase and gain. Both the phase and gain adjustments include fine-tuning potentiometers to ensure maximum precision.\n\u0026lsaquo;\r\u0026rsaquo;\rIt is equipped with BNC inputs and outputs, allowing users to visualize the output on an oscilloscope and achieve maximum attenuation.\nResults This device was compared against a commercial THD meter (GW INSTEK GAD-201G), and the results were highly similar. The primary limitation was the base noise level of the measurement environment, which significantly restricted the lowest THD value we could measure.\nThe specifications of the device are summarized in the following table (in Spanish):\nA detailed analysis of the device is available in this final report (written in spanish).\n","date":"2024-11-26T00:00:00Z","image":"http://localhost:1313/p/thd-n-meter/port_hu1245660591834723805.jpeg","permalink":"http://localhost:1313/p/thd-n-meter/","title":"THD+N Meter"},{"content":"This study was conducted in the context of the class Laboratorio de Acústica at UNTREF. I chose this topic because it aligns with research I have been pursuing as part of the group Intercambios Transorgánicos. The class assignment involved conducting a subjective study using a survey to explore the relationship between objective and subjective variables.\nIn my research group, I have been investigating how denoising algorithms affect Text-to-Speech (TTS) systems trained on low-quality recordings. The focus is on Rioplatense Spanish, a regional accent with limited high-quality data. Within this context, it was natural to combine both tasks and perform a subjective test on the impact of denoising algorithms on TTS systems.\nOverview The key points of this investigation are:\nEvaluation of three denoising algorithms: Wave U-Net, HiFi-GAN, and DeepFilterNet. Use of both subjective (CMOS) and objective metrics (PESQ, STOI, MCD). Insights into resource-efficient TTS model development for underrepresented accents. Methodology Algorithms: Wave U-Net, HiFi-GAN, and DeepFilterNet evaluated with the FastPitch TTS model. Dataset: Subset of the ArchiVoz collection (15 minutes of noisy audio). Testing: CMOS subjective test and objective metrics (PESQ, STOI, MCD). Participants: 24 valid responses, including both experts and non-experts. Key Findings DeepFilterNet Performance:\nAchieved the highest CMOS score, reflecting the best subjective quality. Demonstrated significant improvements in TTS output despite mixed correlations with objective metrics. Objective Metrics Analysis:\nPESQ and MCD showed limited correlation with subjective preferences. STOI scores were consistent across algorithms, indicating preserved intelligibility. Algorithm Comparisons:\nDeepFilterNet: Superior subjective evaluations, moderate MCD. Demucs: Comparable to DeepFilterNet in PESQ but lower subjective scores. Wave U-Net: Poor subjective and objective performance. Subject Expertise:\nNo significant differences were observed between expert and non-expert evaluations in subjective testing. Implications Efficiency: Advanced denoising methods like DeepFilterNet can enhance TTS systems without requiring high-quality recordings. Limitations: Objective metrics like PESQ and MCD are insufficient standalone indicators of subjective TTS quality. Future Work: Expand datasets and noise levels for more robust analysis. Explore TTS systems trained jointly with denoising algorithms. Conclusions This work concludes that preprocessing with DeepFilterNet significantly improves TTS performance, with a 1.1 CMOS score increase. These findings underscore the importance of algorithm selection in optimizing low-resource TTS systems. Additionally, I gained valuable insights into subjective evaluations and the statistical analysis required to draw meaningful conclusions from data.\nAll the information for this study can be found in the academic report.\n","date":"2024-11-22T00:00:00Z","permalink":"http://localhost:1313/p/the-effect-of-denosing-on-tts/","title":"The effect of denosing on TTS"},{"content":"\u0026ldquo;BassAdo\u0026rdquo;: A Semi-Portable Low-Cost Home Speaker This project is part of the Electroacoustics II course at UNTREF within the Sound Engineering program. The task was to design a speaker system from scratch by applying the theory and concepts explained in class. The project was developed over the entire semester, with various stages to complete and present in reports. The speaker is intended for use in large spaces, potentially outdoors, to play music in a social gathering setting. It was named BassAdo to blend the Argentine tradition of \u0026ldquo;asado\u0026rdquo; (a typical barbecue gathering) with the word \u0026ldquo;bass,\u0026rdquo; emphasizing the speaker’s low-frequency performance.\nDesign The goal was to design an accessible home audio system that allowed exploration of topics discussed in the course. The design aimed to emphasize bass response, characteristic of commercial systems, prioritizing low-frequency bandwidth extension over minimal group delay and system time control.\nRegarding the transducers, the team had access to Yharo-brand units, which are classified as non-professional, consumer-grade, and suitable for automotive or home systems. The impedance response of the units was evaluated, and an 8” woofer was selected for low frequencies, along with two 4” units for mid/high frequencies.\nMeasuring the Thiele-Small parameters of the speakers revealed a high Vas (Equivalent Suspension Acoustic Volume), necessitating a large cabinet volume for proper control. To address this, and given the availability of two 8” woofers, the team opted for an isobaric speaker configuration, acoustically coupling the woofers to improve control and reduce cabinet size. Additionally, the cabinet was designed as vented to enhance low-frequency response.\nThe Thiele-Small parameters were obtained using the software REW. With these parameters, simulations were performed in Basta! to optimize the design for the desired response. A key focus was tuning the port’s resonance frequency to achieve strong low-frequency performance. The transducer had an fs of 45 Hz, and the port was tuned to 40 Hz by adjusting the tube length and cabinet air volume.\nBased on these results, a 3D model of the cabinet was created using SolidWorks, and the design was used to cut the materials for construction.\nDetails of this process are documented in the following design report.\nConstruction The wood was cut according to the 3D model, and the cabinet was assembled.\n\u0026lsaquo;\r\u0026rsaquo;\rAs shown in the images, rock wool was added as an acoustic absorber. Measurements revealed this was excessive (the port resonance was overly damped), so some rock wool was removed to achieve the desired result.\nMeasurement and Calibration Frequency response and directivity measurements were conducted in the university’s laboratory using the following equipment:\nPowersoft M50Q amplifier Earthworks M50 microphone RME Fireface UCX audio interface OUTLINE ET250-3D turntable Using this setup and the Arta software, the acoustic response of individual transducers was characterized (useful for crossover filter simulation). Vertical and horizontal directivity responses were also evaluated to determine the best orientation for use. Frequency response graphs for both transducers were generated.\nAll measurements and in-depth analysis are included in the following measurement report.\nCrossover Filter Design Finally, the crossover filter stage was designed. Using the previous measurements, data were uploaded to VituixCad to calculate the simulations. The goal of the crossover filter was to achieve a pleasant frequency response for music playback and to enhance low frequencies. Vertical polar response uniformity was also a priority.\nSince the power stage required an active supply, an active crossover filter with a Sallen-Key topology was implemented. The number of filters was defined based on space and cost, and adjustments were made in the software to achieve the desired response. For example, the low-frequency driver used the following configuration:\nWhere:\nF1: High-pass fs=30 Hz | Q=0.67 F2: Low-pass fs=480 Hz | Q=0.5 F3: Notch filter at 220 Hz F4: Notch filter at 400 Hz Before building the filter, the proposed configuration was tested with a digital filter to practically evaluate the system’s response. Details of this section are provided in the following crossover filter report.\nConclusions This project allowed us to apply theoretical concepts in practice and gain a deeper understanding of the development and challenges involved in designing an electroacoustic system.\n","date":"2024-11-20T00:00:00Z","image":"http://localhost:1313/p/building-and-design-of-a-personal-loudspeaker/front_bass_hu16513420204983711350.PNG","permalink":"http://localhost:1313/p/building-and-design-of-a-personal-loudspeaker/","title":"Building and design of a personal loudspeaker"},{"content":"Automatic Workout Routine Generator This project was the final project asignment for the Algoritmos y Programación II on UNTREF. The idea of the application is to function as a smart workout planification, where the user input certain criterias and the program establish the best workout routine to maximice the users preference. The program was written in the Go programming language.\nData Management This course does not focues on databases, so we choose to use a CSV to simulate a database. This file contains all the information of the different routines and the differente attributes. This serve as a presistency file wheras all the logic is handle by the program.\nDifferent exercises are categorized by tematics. There are two entities on the program that are represented as structs in Go, this are the Exercise and the Routines.\nExercise All the exercises have the following attributes:\nName: Name of the exercise. Description: Detailed description of the exercise. Duration: Estimated duration of the exercise. Calories: Number of calories burned during the exercise. Type: Type of exercise (e.g., cardio, strength, flexibility). Muscle Group: Muscle group targeted by the exercise. Points: Points assigned to the exercise for each of its types. Difficulty: Difficulty level of the exercise. Routines All the routines are a collection of exercises. This are paresed as a linked list of exercise. Aside from this, the routins have the following attributes:\nName: Name of the routine. Exercises: String that stores the IDs of the exercises in the routine, separated by commas. AvailableExercises: Linked list of the exercises available for creating the routine. Algorithm The idea is to maximice different parameters, for example, maximum calories burnt in the least ammount of time, or the minimum duration for this muscle group or type of exercise. To find the best solutions based on the existing data we propose a dynamic programming algorithm that searches for all the posiblities from the data and find the maximum or minimum according to the specifications of the user. The DP apprach uses memory to avoid recalculation combinantions that already were computed, with this optimization the algorithm is fast enough that can generate the routines in miliseconds (with the test dataset under evaluation).\nUser Interface At the moment, the programm is designed to be used form the terminal as a CLI application. The user can crear an exersice and a rutine, list the avaliable optionas, and generate a custom routine base on the specifications that he choose.\nThis is the first iteration of the project and it is fully functional but we plan to create a custom GUI to be user friendly, but for this proposse it will be better to use a different framework and use this GO application as the backend for a web or app service.\nConclusions With this project I solidify my knowledege on programming topics like data structurs and algorithms because we used the theory seen in class and applied it to a real case scenario. Also it help me to get used to the Go langeage and it became a leangauge that I really enjoy coding on it. The code for this project is avaliable on this repository.\n","date":"2024-04-17T00:00:00Z","permalink":"http://localhost:1313/p/automatic-workout-routine-generator/","title":"Automatic Workout Routine Generator"},{"content":"Understanding a modern deep learning model is challenging due to the extensive prior knowledge required and the rapid pace of advancements in the field. In the research project Intercambios Transorgánicos, we are working with TTS, specifically the FastPitch model from Nvidia. I have studied this model to fine-tune it for Spanish and shared my research process in a class to help my colleagues in the research group understand it better.\nUnderstanding Seq2Seq Models In Intercambios Transorgánicos, we previously used Tacotron2 as our TTS model. While Tacotron2 performs well, it has several issues, primarily during training and inference, due to its auto-regressive nature. In contrast, FastPitch is a non-auto-regressive (NAR) model. To grasp these distinctions, I delved into seq2seq models, exploring their evolution over time and creating a quick overview of the sequence analysis models that have been pivotal:\nRNN LSTM Transformers Tacotron2 is based on an LSTM (AR) model, whereas FastPitch utilizes Transformers (NAR). Understanding this technological progression provides crucial background knowledge, particularly about transformers, including positional encoding, a key factor in their non-auto-regressive nature, and the attention mechanism.\nThe Transformer Architecture I began by studying the transformer architecture, as it is fundamental to the FastPitch model. I reviewed online resources and the seminal Attention is All You Need paper. Below are some key points I noted during my study:\nSelf-Attention Mechanism\nPurpose: Dynamically focuses on different parts of the input sequence. Mechanism: Query (Q), Key (K), Value (V): Derived from input embeddings. Attention scores are computed as the dot product of Q and K, scaled by the square root of the dimension. Scores are normalized using softmax to create attention weights. A weighted sum of V is computed based on these weights to produce the output. Multi-Head Attention\nPurpose: Captures diverse relationships between tokens by applying multiple self-attention mechanisms in parallel. Mechanism: Outputs from multiple attention heads are concatenated and linearly transformed. Positional Encoding\nPurpose: Provides information about token order in the sequence, compensating for the Transformer\u0026rsquo;s lack of built-in sequence order (unlike RNNs). Mechanism: A fixed or learnable vector is added to the input embeddings. Encoder-Decoder Structure\nEncoder: Processes the input sequence into context-rich representations. Components: Multi-head self-attention Feed-forward neural network (FFN) Layer normalization and residual connections Decoder: Generates the output sequence by attending to both encoder outputs and previously generated tokens. Components: Masked multi-head self-attention (prevents attending to future tokens) Multi-head attention over encoder outputs FFN, layer normalization, and residual connections Feed-Forward Network (FFN)\nPurpose: Introduces non-linearity and processes each token independently. Mechanism: Two linear layers with a ReLU activation in between. Layer Normalization and Residual Connections\nPurpose: Stabilizes training and improves gradient flow by normalizing inputs to each layer and adding skip connections. FastPitch After covering the theory, I examined each section of the FastPitch architecture in detail. I provided a brief explanation of word embeddings and positional encoding, as these are complex topics, and I wanted to keep the class concise. FastPitch converts text into mel spectrograms, which are then transformed into audio by another model (in our case, HiFi-GAN). The training sequence involves the following steps:\nText to Word Embedding Word Embedding concatenated with mel spectrogram Positional encoding and FFT (Feed-Forward Transformer block) Pitch Prediction Phoneme Duration Prediction Another FFT block Fully connected layer Output mel spectrogram For each block, I presented the corresponding equations and provided qualitative explanations for their roles in the model. For instance, phoneme duration prediction is crucial for aligning a phoneme\u0026rsquo;s duration with the expected duration in the spectrogram.\nOnline Class Finally, I summarized the most important points and conducted an online class to share these concepts with my colleagues. You can watch it here (in Spanish):\n","date":"2023-08-23T00:00:00Z","image":"http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/front_hu1732994514150025714.png","permalink":"http://localhost:1313/p/understanding-fastpitch-and-the-transformer-architecture/","title":"Understanding FastPitch and the Transformer Architecture"},{"content":"On the Procesamiento Digital de Señales (Digital Signal Processing) course at UNTREF, we explore the Z-Plane for filtering design with discrete variables. During the class, the professor introduced a tool built in MATLAB that visualizes the magnitude and phase plots in relation to the positions of zeros and poles on the Z-Plane. Inspired by this, I decided to recreate this tool in Python.\nLeveraging my experience with the PyGame library, I was able to build a real-time application that allows for the movement of poles and zeros, enabling users to see how the transfer function changes in real time.\nHow It Works First, I map the pixel positions on the Z-Plane to coordinates according to the unit circle representation. Then, I construct the transfer function, where each zero $z_{n}$ is a term in the numerator, and each pole $z_{i}$ is a term in the denominator. With the transfer function $H(z)$, I can plot both the magnitude and phase plots, which are also mappings from the normalized response into a space within the app.\n$$ H(z) = \\frac{\\sum_{n=0}^{N} (z - z_{n})}{\\sum_{i=0}^{N} (z - z_{i})}\r$$Each frame recalculates the transfer function. The program performs efficiently because I store the zeros and poles in arrays, enabling faster computations with the help of numpy, which is already highly optimized. This allows real-time visualizations of the changes and provides a more intuitive understanding of the behavior of the Z-Plane. The following code snippet shows how the transfer function is computed using complex exponentials:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def e(w, root): return (np.exp(1j*w) - root) def transfer_function(zeros, poles): w = np.linspace(-np.pi, np.pi, RES) for zero in zeros: num = e(w, zero) for pole in poles: den = e(w, pole) H_z = num/den mag = np.abs(H_z) ang = np.angle(H_z) return mag, ang This project is designed as educational material, providing students with a practical tool to better understand the interactions between poles and zeros in the Z-Plane. It is not intended as a professional filter design tool.\nFunctionality The application has the following functionality:\nDisplay and move poles/zeros: The user can select and choose the position of zero or pole in the Z plane. With the symmetry option active, all the poles and zeros selected or moved are attached to their symmetric par with respect to the imaginary axes.\nOrder: The order of the poles/zeros can be modified with the mouse wheel up to increse or down to decrease. It supports up to order 4. The color of the zero/pole change with the order.\nInformation: Holding the cursor over a pole or zero displays information about the position, symmetry and order.\nZoom: With the plus and minus symbols, the user can zoom in and out of the Z plane.\nDelete: The trash bin symbol clears all the poles and zeros from the plane, and the user can also delete specific poles or zeros by pressing right click on them.\nMagnitude Graph: The magnitude spectrum displayed in the app is normalized. This decision helps to keep the focus on the shape of the magnitude and it makes sure that the graph is visually meaningful for the user. The downside is that the difference in peak values is not captured.\nPhase Graph: The phase is displayed unwrapped between $-\\pi$ and $\\pi$.\nDemo Here is a quick demo of the app in action:\nThe source code for this project can be found on this repo.\n","date":"2023-08-12T00:00:00Z","image":"http://localhost:1313/p/z-plane-visualizer/frontz_hu11319941584083790054.PNG","permalink":"http://localhost:1313/p/z-plane-visualizer/","title":"Z-Plane Visualizer"},{"content":"I used this project as an introduction to Reinforcement Learning. Having mostly worked with supervised and unsupervised learning, I wanted to start with a small, simple project to quickly grasp the main ideas and create something fun. I followed this YouTube video as a reference, which explains how to use Reinforcement Learning (RL) to train a model capable of playing the snake game. To make it more challenging, I applied the same network to the 2048 game.\nCode The code has three main components:\nThe Game: Implements the game logic and the graphical user interface (GUI). Agent: Controls the gameplay. AI Model: A neural network that learns how to play and guides the agent. Game State I modeled the game state as a 4x4 matrix representing the board. The actions in the game are represented by a vector, with the following possibilities:\nLeft: [1, 0, 0, 0] Right: [0, 1, 0, 0] Up: [0, 0, 1, 0] Down: [0, 0, 0, 1] Reward The RL model works with rewards to quantify when the agent performs well or poorly. Initially, I defined the following rewards:\n+10 points: When the agent doubles the points. This is crucial because, in 2048, points increase exponentially. -10 points: When the game is lost. This serves as a straightforward negative reward. Initially, the model showed slow improvement. To address this, I added an additional reward:\n+2 points: When points are increased. This reward incentivizes actions that maximize board clearance and progress. How the Model Works The model is a simple linear neural network. It takes the game state as input and predicts the best next action to maximize the reward.\nRewards are managed using a technique called Q-Learning. A Q Value represents the quality of a decision based on the loss function. The loss function is derived from the Bellman Equation:\n$$\rNewQ(s, a) = Q(s, a) + \\alpha [R(s, a) + \\lambda \\, \\text{max}Q'(s', a') - Q(s, a)]\r$$Where:\n$Q(s, a)$: The Q Value for a specific state and action. $\\alpha$: Learning rate. $R(s, a)$: Reward for a specific state and action. $\\lambda$: Discount rate. $\\text{max}Q\u0026rsquo;(s\u0026rsquo;, a\u0026rsquo;)$: Maximum expected future reward. Experiments and Results Random Test as a Baseline To establish a baseline, I tested the average score achievable by taking random actions. After 1,000 iterations, the average score was 170, far below the 2048 points needed to win the game.\nInitial Results My initial attempts were discouraging. The model performed worse than random movements. Here are some early results:\nGame 1,000 | Score: 208 | Record: 416 | Mean Score: 200 | Reward: 640 Game 1,000 | Score: 116 | Record: 346 | Mean Score: 161 | Reward: 310 Game 1,000 | Score: 112 | Record: 348 | Mean Score: 146 | Reward: 320 In these attempts, the agent developed a suboptimal strategy of filling the board before increasing points.\nImprovements After experimenting with the reward parameters, I focused on the exploration phase. Initially, the exploration games parameter, which randomly picks moves, was set to 25 games. Increasing this parameter allowed the agent to explore more strategies, leading to better results:\nGame 1,000 | Score: 478 | Record: 478 | Mean Score: 230 Game 1,000 | Score: 514 | Record: 964 | Mean Score: 382 As the model improved, I extended the training to more games:\nGame 3,796 | Score: 770 | Record: 1,366 | Mean Score: 469 Game 4,852 | Score: 631 | Record: 1,462 | Mean Score: 483 Finally, with 200 exploration games and 5,000 training games, the results were as follows:\nGame 5,000 | Score: 840 | Record: 1,678 | Mean Score: 512 Although the model didn\u0026rsquo;t beat the game, I was satisfied with the progress. I believe that with longer training (the final run lasted 4 hours) and additional network layers, it would be possible to win the game using this architecture.\nDemo Here is a demo of the application, showcasing the agent\u0026rsquo;s learning process:\nI implemented keybindings to control the game\u0026rsquo;s speed, providing three options:\nFast: For rapid training. Slow: To observe and analyze the agent\u0026rsquo;s progress and mistakes. Medium: Rarely used. The complete code for this project is available in this repository.\n","date":"2023-02-17T00:00:00Z","image":"http://localhost:1313/p/ai-learns-to-play-2048-game/game_img_hu1650208459137967495.jpg","permalink":"http://localhost:1313/p/ai-learns-to-play-2048-game/","title":"AI learns to play 2048 game"},{"content":"Time scale modifications algorithms are used to speed up or slow down the reproduction velocity of an audio. When you change the sample rate of an audio, the velocity changes but it also changes the pitch (when the audio is speed up it sounds highier pitch). There are different algorithms that change the velocity of the audio but mantain the pitch.\nThe main reference for this study is the following article, where all the different algorithms are describe in detail.\nA Review of Time-Scale Modification of Music Signals.\n— Jonathan Driedger and Meinard Müller1\nAlgorithm comparison There are two main algorithms, the Overlap-and-add (OLA) and the Phase Vocoder (PV). Both achieve good results under different signals and conditions. For this, a final implementation using Harmonic Percussion Separation (HPS) combines both algorithms and achiving the best results.\nOLA This method works on time domain and overlap sections of the audio (windows) and rearange it to achive a certain desire change on speed. This method works well for percussive signals, but it introduces artifacts when used with harmonic or tonal signals.\nPV This method works on frequency domain, and it combines chunks of audio in the frequency domain to achieve the desire change in time. This uses the phase vocoder principle to propagate the phase between the windows, this grantice the continuity of when applied to harmonic signals. On the contrary, it does not work for percussive signals becouse the phase propagation process eliminate the transients in the signals.\nI created visualizations using Manim to enhance my class presentation. The first video demonstrates how the PV algorithm aligns windows to ensure smooth transitions in the generated signal over time. To achieve this, a Gaussian window is applied, which maintains continuity and smoothness, even at the start and end of the sequence.\nThe second video showcases the effects of applying the PV algorithm to a signal containing transients.\nAs predicted by theory, the transients vanish because the PV algorithm disrupts the vertical phase alignment. While these examples utilize idealized signals, they effectively demonstrate the key strengths and limitations of the algorithm.\nHPS To use both methods with their ideal signals, the HPS algorithms is used. This algorithm separete the signal into the harmonics and the percussive parts. It works by comparing the continuty of the signal in the STFT representation and using a filter to compare vertical versus horizontal presence on the spectrogram. With a threshold, a binary mask can be define over the spectrogram to separete the percussive parts from the harmoincs sections.\nResults We succesfully implement all the algorithms and compare them, verifying the theortical contents presented on the referenca article. In the process, we develope the toolkit to use this algorithms with the python programming language. All the code is avaliable on this repo\nAcademic Presentation The study was preseented with my classmates on the JAAS (Jornadas de Acustica, Audio y Sonido). The main ideas and conclusions were preseneted on the conference. In the following repoert there is all the details and anlysis done for this proyect (in spanish).\nALGORITMOS DE MODIFICACIÓN DE ESCALA TEMPORAL.\n— Matías Di Bernardo; Matías Vereertbruhggen; Sebastían Carro 2\nA Review of Time-Scale Modification of Music Signal paper.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJAAS 2023 - Algoritmos de Modificación de Escala Temporal paper.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-02-11T00:00:00Z","image":"http://localhost:1313/p/time-scale-modification-algorithms/tsm_hu16003083865389877412.PNG","permalink":"http://localhost:1313/p/time-scale-modification-algorithms/","title":"Time Scale Modification Algorithms"},{"content":"This research is conducted as part of the subject Metodología de la Investigación at UNTREF. The article aims to compare the differences between three types of time-frequency transformations:\nFourier Transform (FT) Wavelet Transform (WT) Huang-Hilbert Transform (HHT) The objective of this work is to understand the differences between these types of transformations and deepen my knowledge of signal processing.\nObjective The general objective of the research is to determine which spectral analysis tool achieves the highest accuracy in pitch detection tasks.\nTo achieve this objective, the following specific objectives are proposed:\nIdentify the parameters needed to represent the signal in the spectral domain for each case. Select an algorithm that identifies the pitch of the signal based on its spectral representation. Generate the data (audio signals) to be used for the comparison. Evaluate the generated data with the different analysis methods and apply statistical processes to validate the results. Establish a measure of accuracy for the pitch detection task. Compare the results of the different analyses and determine which method achieves the highest accuracy in pitch detection. The pitch detection task was chosen because it is one of the main applications of these types of transformations.\nAlgorithms The theoretical analysis of all the transformations is performed in the continuous domain, but to conduct the experiments and comparisons, the discrete domain is used, enabling all calculations to be performed digitally.\nFFT The FFT is an algorithm that optimizes the DFT (Discrete Fourier Transform). With this algorithm, the spectral representation of the signal is obtained according to Fourier analysis, which decomposes a complex signal into a sum of sines or cosines. The DFT is calculated using the formula:\n$$\rX_k = \\sum_{n=0}^{N-1} e^{-i\\frac{2\\pi}{N}kn} x_n\r$$Where \\( N \\) is the number of signal samples, and \\( k \\) are natural numbers from \\( 0 \\) to \\( N – 1 \\).\nWT The Wavelet Transform (WT) uses an oscillatory function (wavelet) and applies a convolution between the signal and the chosen wavelet function to determine whether that wave shape is present in the signal. The wavelet is stretched and scaled in frequency and amplitude, allowing a single wavelet function to recreate the entire spectrum of interest.\nIn this research, the CDWT (Cyclic Discrete Wavelet Transform) will be used, the most common implementation when discretizing the WT. Conceptually, this transform extends Fourier analysis by projecting the signal onto a basis of wavelet functions instead of sines and cosines. It is calculated as follows:\n$$\rWf[n, a^j] = \\sum_{m=0}^{N-1} f[m] \\psi_j[m-n]\r$$Where \\( N \\) is the number of signal samples, \\( \\psi \\) is the wavelet function, and \\( j \\) represents the deformation of the wavelet according to the selected wavelet bank.\nHHT Lastly, the Huang-Hilbert Transform (HHT) will be used for spectral representation. It employs a method called Empirical Mode Decomposition (EMD) to decompose the signal into subsignals that contain the relevant information of the original function.\nLike the previous analysis methods, the key part of the analysis is the decomposition of the signal into simpler signals. However, instead of sine or wavelet functions, EMD finds intrinsic mode functions (IMFs) that form the basis of our decomposition and are unique to each signal.\nThe relationship between the IMFs and the frequency of the original signal is established with the equation:\n$$\rz(t) = f(t) + i H\\{ f(t) \\}\r$$Where \\( f(t) \\) is an IMF of the original signal, and \\( H \\) is the Hilbert Transform. This allows the IMF to be represented as a complex signal by projecting it onto the imaginary axis using the Hilbert Transform.\nThus, the IMF is represented as a complex signal, and the amplitude and phase of each moment can be extracted to construct the spectral representation. Since a signal generally has multiple IMFs, this process is repeated for all of them, and the results are summed to obtain the complete spectrum.\nProcedure This research analyzes the relationship between types of spectral representation and accuracy in pitch detection.\nFirst, the parameters for the different transformations will be selected. Among the most critical parameters to determine is the number of samples for temporal windowing, as it determines the trade-off between temporal and frequency resolution.\nTo ensure a fair comparison among the methods, data representing various cases of interest will be generated, including four types of signals:\nMonophonic: Signals with a single note corresponding to \\( F_0 \\). Polyphonic: Signals with multiple notes where harmony determines \\( F_0 \\). Slow Transitions: Signals with gradual changes in \\( F_0 \\). Fast Transitions: Signals with abrupt changes in \\( F_0 \\). The real value \\( V(t) \\) will be compared to the result \\( P(t) \\) from each transform, integrating the difference over time to calculate the accuracy.\nResults At this stage, the task was to complete the research plan detailing the procedure and analysis methods. Dummy data was generated and statistically validated to simulate expected results.\nThe graph compares the precision achieved by the three transformations for different signal types. Based on the properties of the transforms, the Wavelet Transform (WT) is expected to outperform the Fourier Transform (FT), and the Huang-Hilbert Transform (HHT) is expected to achieve the highest precision overall.\nConclusions In pitch detection tasks using spectral analysis, the Huang-Hilbert Transform (HHT) generally provides higher precision than the Fast Fourier Transform (FFT) and the Cyclic Discrete Wavelet Transform (CDWT).\nThe significance of this precision gain depends on the type of signal being analyzed, with fast-transition signals benefiting the least from the transformation change, while polyphonic signals show the most significant improvement when using the HHT.\nThis project allowed me to deepen my understanding of signal processing and grasp the foundations of why tools like the WT and HHT are used based on the characteristics of the signal being analyzed.\nAll details of this work are available in the following report.\n","date":"2022-11-11T00:00:00Z","image":"http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/fourier_hu9759776206134608272.jpg","permalink":"http://localhost:1313/p/comparative-analysis-of-time-frequency-transformations/","title":"Comparative analysis of time-frequency transformations"},{"content":"This project started as the final assignment for a seminar class at UNTREF called Seminario en Aplicaciones de Redes Neuronales en la recuperación de información musical. The objective was to use a Siamese Neural Network (SNN) in a different context than the one explored in class (music similarity detection).\nFor the final project of this course, we developed an SNN model from scratch using the Keras framework and the SincNet architecture to reduce audio dimensionality, achieving good results. Later, to expand this project, I tried another approach by using Wav2Vec for dimensionality reduction and re-implementing the entire project in the PyTorch framework. This attempt, however, yielded suboptimal results, indicating that the dimensionality reduction using Wav2Vec lost critical information required for the speaker diarization task.\nSpeaker Diarization Task The goal of a speaker diarization model is to identify different speakers in an audio stream containing multiple speakers. For example, in a podcast with two people (A and B), the model must determine the time steps where speaker A is talking and the time steps where speaker B is speaking (and implicitly identify the periods of silence). These models are incredibly useful for audio editing and analyzing long audio sequences with multiple speakers.\nWhy Siamese Neural Networks? In class, we explored the Siamese architecture to compare similarities between pieces of music, developing a tool capable of identifying covers of famous songs.\nA Siamese Neural Network consists of two or more identical subnetworks sharing the same weights and parameters. It is designed to compare input pairs and measure their similarity, typically using a distance metric like Euclidean distance. Each subnetwork processes one input, and the outputs are combined to compute a similarity score.\nWith this similarity comparison in mind, we wanted to apply these networks to the speaker diarization task. The idea was to generate speaker embeddings from audio using a pre-trained model and compare the outputs of different audio segments. Based on the similarity score, we aimed to identify the segments where different speakers are talking.\nExperiments I tested two different methods for audio feature extraction to serve as speaker embeddings.\nKeras Implementation with SincNet In this approach, we used the SincNet architecture to extract speaker-specific features from audio. SincNet applies learnable sinc functions as its filters, which are particularly well-suited for audio processing as they mimic traditional bandpass filters. These features were then fed into the Siamese Neural Network, which compared pairs of audio segments to calculate their similarity scores. The model was trained on labeled audio datasets, and we observed strong performance in clustering audio segments by speaker, achieving clear boundaries between different speakers.\nA report with the results can be found in the following Jupyter notebook (in Spanish).\nPyTorch Implementation with Wav2Vec For this method, I utilized Wav2Vec, a powerful pre-trained model for extracting deep audio embeddings. Unlike SincNet, Wav2Vec embeddings are derived from self-supervised learning, capturing high-level representations of audio. These embeddings were used in the Siamese Neural Network for similarity comparisons. However, the results were suboptimal for the diarization task. It appears that Wav2Vec, while excellent for speech recognition tasks, lost some speaker-specific details necessary for distinguishing between speakers in our setup.\nResults The experiments demonstrated that the choice of feature extraction method is crucial for speaker diarization. The Keras implementation with SincNet outperformed the PyTorch implementation with Wav2Vec, showing higher accuracy in identifying speaker transitions. This suggests that task-specific feature extraction, like SincNet, is more effective than general-purpose embeddings like Wav2Vec for speaker diarization.\nThe code for this project is available in this repository.\nConclusions This project was one of my first experiences with deep learning models, where I applied my knowledge to a problem without following a specific paper or using a pre-trained model. I explored different solutions and concluded on the importance of feature extraction and model selection.\nIt also helped me become familiar with the syntax of the most popular deep learning frameworks and solidified my understanding in the process.\n","date":"2022-11-04T00:00:00Z","permalink":"http://localhost:1313/p/evaluation-of-different-models-for-speaker-diarization-task/","title":"Evaluation of different models for Speaker Diarization task"},{"content":"This project is the final assignment for the class Acoustics and Psychoacoustics II, where we were tasked with redesigning an existing auditorium. The goal was to apply the theory covered in class to create an acoustically optimized auditorium. For our project, we chose to redesign the Royal Albert Hall in London. This was particularly challenging due to the auditorium\u0026rsquo;s vast dimensions, which make it difficult to ensure that sound reaches all spectators equally.\nRedesign Main Ideas The redesign aimed to preserve the original concept of the auditorium, including its large volume and extensive seating capacity, while introducing critical changes to improve its acoustics. Although the primary focus was on acoustic enhancement, the redesign also considered other essential factors, such as sightlines and appropriate seat distribution.\nDespite the intent to maintain the auditorium\u0026rsquo;s original dimensions, its volume proved too large to achieve an optimal reverberation time. To address this, the redesign introduced an intermediate ceiling to reduce the spherical ceiling\u0026rsquo;s volume, and the main seating area was reduced. These changes helped create a better reverberation time in the room, as illustrated in the cross-section below.\nBuilding Details and Regulations To ensure a feasible and functional redesign, the following key aspects were carefully considered:\nSeat distribution Corridor spacing Sightline optimization Stage comfort Acoustic Treatment Acoustic treatment was the most critical part of this study and focused on two main aspects: reflections and reverberation time.\nReflections Analyzing reflections is essential for the audience\u0026rsquo;s acoustic experience. The original Royal Albert Hall features a spherical ceiling that centralizes reflections, creating undesirable acoustic effects. To mitigate this, the redesign incorporated an intermediate ceiling with a specific geometry designed to distribute reflections evenly across the audience.\nThe staggered ceiling design ensures adequate reflections for all seating rows. In the main balcony, two reflections were specifically addressed to compensate for the lower sound pressure level (SPL) caused by the large distance from the stage, as shown in the image below.\nLateral reflections were also optimized through adjustments to the stage geometry and the walls of the lateral balconies.\nAdditionally, the redesign sought to minimize the Initial Time Delay Gap (ITDG) across different audience locations.\nMaterials and Reverberation Time The redesign adhered to recommendations from Acoustic Absorbers and Diffusers to achieve a balance between absorption, diffusion, and specular reflections. Reflective materials were used for the ceiling and parts of the lateral balconies to ensure effective specular reflections. To lower the reverberation time (RT), materials with higher absorption coefficients were applied to other surfaces.\nUsing the selected materials and the Sabine equation, we calculated the auditorium\u0026rsquo;s estimated RT. The resulting reverberation time for different frequencies is shown below:\nThe calculated mid-frequency RT is 2.51 seconds. While this is slightly above the recommended maximum of 2.4 seconds for optimal acoustics, it is acceptable given the auditorium\u0026rsquo;s large volume.\n3D Modelling We rendered the redesigned auditorium using SketchUp software. Below are some of the visualizations:\n\u0026lsaquo;\r\u0026rsaquo;\rConclusions Redesigning the Royal Albert Hall to improve its acoustics while retaining its original essence presented significant challenges. The project required innovative solutions to address acoustic issues without compromising the hall\u0026rsquo;s iconic design. Although some changes were necessary, the final result demonstrates a thoughtful redesign that enhances acoustics while preserving the auditorium\u0026rsquo;s historical character. This project also deepened our understanding of acoustics and auditorium design principles.\nA detailed description of this project can be found in the following article.\n","date":"2022-06-22T00:00:00Z","image":"http://localhost:1313/p/theather-acoustic-design/front_hu7206688613610463615.PNG","permalink":"http://localhost:1313/p/theather-acoustic-design/","title":"Theather Acoustic Design"}]